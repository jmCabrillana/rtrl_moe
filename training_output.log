Device: cuda

================================================================================
Experiment: sparse_moe_rtrl_1m_20251207_192024
================================================================================
Sequence length: 128 tokens
Model: MoE (d=32, slots=4, experts=4, topk=2)
Regularization: Lyapunov=0.001, Expert norm=0.001
Logs: runs/sparse_moe_rtrl_1m_20251207_192024
Checkpoints: checkpoints/sparse_moe_rtrl_1m_20251207_192024

Starting training on 1M-token sequences...

Traceback (most recent call last):
  File "/home/jm/Development/rtrl_moe/train_stable_1m.py", line 194, in <module>
    rtrl.step(model, x_t, h_t, task_loss, active_params, read_idx, write_idx)
  File "/home/jm/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
  File "/home/jm/Development/rtrl_moe/rtrl_block.py", line 164, in step
    dL_dTheta = dict(zip(params.keys(), torch.autograd.grad(loss, params.values(), retain_graph=True, allow_unused=True)))
  File "/home/jm/.local/lib/python3.10/site-packages/torch/autograd/__init__.py", line 503, in grad
    result = _engine_run_backward(
  File "/home/jm/.local/lib/python3.10/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.
