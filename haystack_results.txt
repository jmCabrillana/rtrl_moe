===========================================================================
HAYSTACK TASK - Progressive Testing
===========================================================================

===========================================================================
PHASE 1: Test BPTT Convergence on Different Sequence Lengths
===========================================================================

--- Tiny (len=8, vocab=8) ---
  Training RNN... 100 200 300 400 500
  RNN:  Loss = 0.002, Acc = 100.0%
  Training MoE... 100 200 300 400 500
  MoE:  Loss = 0.000, Acc = 100.0%
  âœ“ MoE converges on this task!

--- Short (len=12, vocab=10) ---
  Training RNN... 100 200 300 400 500
  RNN:  Loss = 0.014, Acc = 100.0%
  Training MoE... 100 200 300 400 500
  MoE:  Loss = 1.891, Acc = 10.0%
  âœ— Too difficult - MoE doesn't learn well

--- Medium (len=16, vocab=12) ---
  Training RNN... 100 200 300 400 500
  RNN:  Loss = 0.451, Acc = 94.0%
  Training MoE... 100 200 300 400 500
  MoE:  Loss = 2.212, Acc = 6.0%
  âœ— Too difficult - MoE doesn't learn well

âœ“ Best config: Tiny (len=8, vocab=8) (Acc=100.0%)
   Using this for RTRL tests...

===========================================================================
PHASE 2: Test RTRL on Working Configuration
===========================================================================

Task: Tiny (len=8, vocab=8)
Training MoE with RTRL for 1000 steps...
  Progress:/home/ubuntu/rtrl_moe/.venv/lib/python3.12/site-packages/torch/nn/functional.py:6487: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_flash_attention_for_cpu. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /pytorch/aten/src/ATen/functorch/BatchedFallback.cpp:81.)
  attn_output = scaled_dot_product_attention(
/home/ubuntu/rtrl_moe/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_flash_attention_for_cpu_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /pytorch/aten/src/ATen/functorch/BatchedFallback.cpp:81.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
.. [100: Loss=1.576, Acc=30.0%].. [200: Loss=1.432, Acc=34.0%].. [300: Loss=1.464, Acc=26.0%].. [400: Loss=1.481, Acc=26.0%].. [500: Loss=1.525, Acc=22.0%].. [600: Loss=1.523, Acc=36.0%].. [700: Loss=1.693, Acc=18.0%].. [800: Loss=1.431, Acc=22.0%].. [900: Loss=1.411, Acc=22.0%].. [1000: Loss=1.388, Acc=24.0%]

RTRL Results: Loss = 1.388, Acc = 24.0%

===========================================================================
PHASE 3: Segment Tree Speedup Test
===========================================================================
(Testing speedup regardless of RTRL accuracy)
Running 15 steps with SPARSE + LAZY (segment tree)...
Running 15 steps with SPARSE (NO lazy updates)...

Results:
  Sparse + Lazy:  6.02s
  Full updates:   9.09s
  Speedup:        1.51x
âœ“ Segment tree provides speedup!

âš  RTRL achieves 24.0% (BPTT: 100%) - needs tuning

===========================================================================
SUMMARY
===========================================================================
âœ“ Found working configuration: Tiny (len=8, vocab=8)
  - MoE with BPTT: 100% accuracy âœ“
  - RTRL: 24.0% accuracy (lower than BPTT)
  - Segment tree speedup: 1.51x âœ“

ðŸŽ‰ HAYSTACK VERIFICATION COMPLETE
