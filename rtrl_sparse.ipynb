{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8c6b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_rmt_partial_softk.py\n",
    "# Recurrent Memory Transformer with differentiable soft top-k bank routing\n",
    "# and partial memory updates (only a subset of memory slots, and optionally dims, update per chunk).\n",
    "\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --------------- Utilities ---------------\n",
    "\n",
    "def set_seed(seed: int = 1337):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def fourier_pos_enc(pos, d, base=10_000.0):\n",
    "    # pos: [T,1] or [B,T,1] (float), returns [T,d] (or broadcastable)\n",
    "    if pos.dim() == 3:\n",
    "        pos = pos[0]\n",
    "    half = d // 2\n",
    "    if half == 0:\n",
    "        return torch.zeros_like(pos)\n",
    "    freqs = torch.arange(half, device=pos.device).float() / max(1, half)\n",
    "    ang = pos.float() / (base ** freqs)  # [T,half]\n",
    "    return torch.cat([torch.sin(ang), torch.cos(ang)], dim=-1)  # [T,d]\n",
    "\n",
    "# --------------- Model ---------------\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, d, mult=4, p=0.0):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d, mult * d)\n",
    "        self.fc2 = nn.Linear(mult * d, d)\n",
    "        self.drop = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.drop(F.gelu(self.fc1(x))))\n",
    "\n",
    "class RMTBlockPartial(nn.Module):\n",
    "    \"\"\"One layer: token pass (reads full prev memory), then memory update (writes only selected banks).\"\"\"\n",
    "    def __init__(self, d, n_heads, p=0.0, d_proj=None):\n",
    "        super().__init__()\n",
    "        self.ln_tok_q  = nn.LayerNorm(d)\n",
    "        self.ln_tok_kv = nn.LayerNorm(d)\n",
    "        self.mha_tok   = nn.MultiheadAttention(d, n_heads, dropout=p, batch_first=True)\n",
    "        self.ff_tok    = MLP(d, p=p)\n",
    "        self.ln_tok_ff = nn.LayerNorm(d)\n",
    "\n",
    "        self.ln_mem_q  = nn.LayerNorm(d)\n",
    "        self.ln_mem_kv = nn.LayerNorm(d)\n",
    "        self.mha_mem   = nn.MultiheadAttention(d, n_heads, dropout=p, batch_first=True)\n",
    "        self.ff_mem    = MLP(d, p=p)\n",
    "        self.ln_mem_ff = nn.LayerNorm(d)\n",
    "\n",
    "        self.d_proj = d_proj  # if set, update only first d_proj dims of chosen bank(s)\n",
    "\n",
    "    def token_pass(self, tok, mem, mask_tok):\n",
    "        q  = self.ln_tok_q(tok)\n",
    "        kv = self.ln_tok_kv(torch.cat([mem, tok], dim=1))  # [B,M+T,D]\n",
    "        attn, _ = self.mha_tok(q, kv, kv, need_weights=False, attn_mask=mask_tok)\n",
    "        tok = tok + attn\n",
    "        tok = tok + self.ff_tok(self.ln_tok_ff(tok))\n",
    "        return tok\n",
    "\n",
    "    def bank_update(self, mem, tok, bank_slices, bank_weights):\n",
    "        \"\"\"\n",
    "        mem: [B,M,D], tok: [B,T,D]\n",
    "        bank_slices: list of (start, end) for each chosen bank per sample (length = topk per sample)\n",
    "                     shape handled with a small loop over (B, topk) for clarity.\n",
    "        bank_weights: [B, topk] non-negative weights (softmax probs for chosen banks).\n",
    "        \"\"\"\n",
    "        B, M, D = mem.shape\n",
    "        mem_next = mem\n",
    "        dproj = self.d_proj\n",
    "\n",
    "        # small, robust loops (B and topk are modest)\n",
    "        for b in range(B):\n",
    "            for i, (start, end) in enumerate(bank_slices[b]):\n",
    "                w = bank_weights[b, i]\n",
    "                if w.item() == 0.0:\n",
    "                    continue\n",
    "                sl = slice(start, end)  # size group\n",
    "                qm  = self.ln_mem_q(mem_next[b:b+1, sl, :])  # [1,group,D]\n",
    "                kvm = self.ln_mem_kv(torch.cat([mem_next[b:b+1], tok[b:b+1]], dim=1))\n",
    "                upd, _ = self.mha_mem(qm, kvm, kvm, need_weights=False)   # [1,group,D]\n",
    "                upd = upd + self.ff_mem(self.ln_mem_ff(upd))              # [1,group,D]\n",
    "                if dproj is not None and dproj < D:\n",
    "                    mem_next[b, sl, :dproj] = mem_next[b, sl, :dproj] + w * upd[0, :, :dproj]\n",
    "                else:\n",
    "                    mem_next[b, sl, :]     = mem_next[b, sl, :]     + w * upd[0, :, :]\n",
    "        return mem_next\n",
    "\n",
    "class BankGater(nn.Module):\n",
    "    \"\"\"Soft bank router producing logits over K banks; temperature is applied in forward.\"\"\"\n",
    "    def __init__(self, d, K, hidden=None, p=0.0):\n",
    "        super().__init__()\n",
    "        h = hidden or (2 * d)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(d),\n",
    "            nn.Linear(d, h),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(h, K)\n",
    "        )\n",
    "        self.tau = 1.0  # temperature (adjust in trainer)\n",
    "\n",
    "    def forward(self, feat):\n",
    "        logits = self.net(feat)              # [B,K]\n",
    "        tau = max(1e-3, float(self.tau))\n",
    "        probs = torch.softmax(logits / tau, dim=-1)\n",
    "        return probs, logits\n",
    "\n",
    "class RMTPartialSoftK(nn.Module):\n",
    "    \"\"\"\n",
    "    Memory M split into K banks of size group = M//K.\n",
    "    Each chunk: tokens read full mem_prev; router picks per-sample top-k banks with soft weights;\n",
    "    only those banks are updated (optionally only first d_proj dims).\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab, d=256, n_heads=4, n_layers=4, M=128, K=8, d_proj=None, p=0.0):\n",
    "        super().__init__()\n",
    "        assert M % K == 0, \"M must be divisible by K\"\n",
    "        self.d, self.M, self.K = d, M, K\n",
    "        self.group = M // K\n",
    "        self.embed = nn.Embedding(vocab, d)\n",
    "        self.pos_scale = nn.Parameter(torch.ones(1))\n",
    "        self.blocks = nn.ModuleList([RMTBlockPartial(d, n_heads, p, d_proj) for _ in range(n_layers)])\n",
    "        self.ln_out = nn.LayerNorm(d)\n",
    "        self.out = nn.Linear(d, vocab)\n",
    "\n",
    "        self.gater = BankGater(d, K, hidden=2*d, p=p)\n",
    "\n",
    "        # tiny aux weights (can be 0.0 during warmup)\n",
    "        self.lb_lambda = 0.0  # load-balance\n",
    "        self.ent_lambda = 0.0  # (optional) encourage entropy (diversity)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def init_mem(self, B, device=None, dtype=None):\n",
    "        device = device or next(self.parameters()).device\n",
    "        dtype  = dtype  or next(self.parameters()).dtype\n",
    "        return torch.zeros(B, self.M, self.d, device=device, dtype=dtype)\n",
    "\n",
    "    def token_mask(self, T, device, dtype):\n",
    "        # allow token->mem; causal token->token; avoid -inf (use large negative)\n",
    "        mask = torch.zeros(T, self.M + T, device=device, dtype=dtype)\n",
    "        neg = torch.full((T, T), -1e4, device=device, dtype=dtype)\n",
    "        mask[:, self.M:] = torch.triu(neg, 1)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x, mem_prev, pos_offset=0, topk=2, collect_aux=True):\n",
    "        \"\"\"\n",
    "        x: [B,T] token ids\n",
    "        mem_prev: [B,M,D]\n",
    "        Returns: logits [B,T,V], mem_next [B,M,D], aux dict\n",
    "        \"\"\"\n",
    "        B, T = x.shape\n",
    "        tok = self.embed(x)                                    # [B,T,D]\n",
    "        pos = torch.arange(pos_offset, pos_offset + T, device=x.device, dtype=tok.dtype).unsqueeze(-1)\n",
    "        tok = tok + self.pos_scale * fourier_pos_enc(pos, self.d).to(tok.dtype)\n",
    "\n",
    "        mask_tok = self.token_mask(T, x.device, tok.dtype)\n",
    "\n",
    "        # router features (pre-update)\n",
    "        pool = tok.mean(dim=1)                                 # [B,D]\n",
    "        probs, logits = self.gater(pool)                       # [B,K]\n",
    "        topk = min(topk, self.K)\n",
    "        pvals, pidx = torch.topk(probs, topk, dim=-1)          # [B,topk], [B,topk]\n",
    "\n",
    "        # Build per-sample bank slices\n",
    "        bank_slices = []\n",
    "        for b in range(B):\n",
    "            slices_b = []\n",
    "            for i in range(topk):\n",
    "                g = int(pidx[b, i].item())\n",
    "                start = g * self.group\n",
    "                slices_b.append((start, start + self.group))\n",
    "            bank_slices.append(slices_b)\n",
    "\n",
    "        mem = mem_prev\n",
    "        # Pass through blocks: token pass first, then weighted bank updates\n",
    "        for blk in self.blocks:\n",
    "            tok = blk.token_pass(tok, mem, mask_tok)\n",
    "            mem = blk.bank_update(mem, tok, bank_slices, pvals)\n",
    "\n",
    "        logits_tok = self.out(self.ln_out(tok))\n",
    "\n",
    "        aux = {}\n",
    "        if collect_aux:\n",
    "            # Encourage average probs across batch to be ~uniform (rough load-balance)\n",
    "            avg_p = probs.mean(dim=0)                                  # [K]\n",
    "            uni = torch.full_like(avg_p, 1.0 / self.K)\n",
    "            lb = torch.sum(avg_p * (avg_p.add(1e-8).log() - uni.add(1e-8).log()))  # KL(avg||uni)\n",
    "            ent = (-(probs * (probs.add(1e-8).log())).sum(dim=-1).mean())\n",
    "            aux = {\n",
    "                \"lb_loss\": self.lb_lambda * lb,\n",
    "                \"ent_bonus\": self.ent_lambda * ent,\n",
    "                \"gate_entropy\": float(ent.detach().item())\n",
    "            }\n",
    "\n",
    "        return logits_tok, mem, aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b726a97",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Non-finite detected. Lower LR, check mask, or print intermediates.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 104\u001b[0m\n\u001b[1;32m    101\u001b[0m             model\u001b[38;5;241m.\u001b[39mgater\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, model\u001b[38;5;241m.\u001b[39mgater\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.95\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 81\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m         lb \u001b[38;5;241m=\u001b[39m eb \u001b[38;5;241m=\u001b[39m ce\u001b[38;5;241m.\u001b[39mnew_tensor(\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m     80\u001b[0m     loss_step \u001b[38;5;241m=\u001b[39m ce \u001b[38;5;241m+\u001b[39m lb \u001b[38;5;241m+\u001b[39m eb\n\u001b[0;32m---> 81\u001b[0m     \u001b[43mfinite_or_die\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m+\u001b[39m loss_step\n\u001b[1;32m     84\u001b[0m global_pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m T_total\n",
      "Cell \u001b[0;32mIn[11], line 51\u001b[0m, in \u001b[0;36mmain.<locals>.finite_or_die\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misfinite(t)\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m---> 51\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-finite detected. Lower LR, check mask, or print intermediates.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Non-finite detected. Lower LR, check mask, or print intermediates."
     ]
    }
   ],
   "source": [
    "\n",
    "# --------------- Minimal Trainer ---------------\n",
    "\n",
    "def make_stream_batch(B, T_total, V, D, device):\n",
    "    x = torch.randint(0, V, (B, T_total), device=device)\n",
    "    y = torch.full((B, T_total), -100, device=device)  # ignore first D tokens\n",
    "    y[:, D:] = x[:, :-D]\n",
    "    return x, y\n",
    "\n",
    "def main():\n",
    "\n",
    "    device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    VOC      = 8\n",
    "    DELAY    = 64\n",
    "    CHUNK    = 32           # can be < DELAY\n",
    "    BATCH    = 32\n",
    "    LAYERS   = 3\n",
    "    DIM      = 128\n",
    "    HEADS    = 4\n",
    "    M        = 128          # M >= DELAY + margin\n",
    "    K        = 8            # number of banks; only top-k banks (per sample) updated\n",
    "    TOPK     = 2\n",
    "    D_PROJ   = 64           # update only first D_PROJ dims in chosen bank(s); None -> full D\n",
    "    LR       = 1e-4\n",
    "    STEPS    = 1200\n",
    "    WARM_STEPS = 300        # during warmup: no aux losses, higher router temperature\n",
    "    ACCUM    = 1            # gradient accumulation episodes per optimizer step\n",
    "    LOG_EVERY = 20\n",
    "\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    model = RMTPartialSoftK(\n",
    "        vocab=VOC, d=DIM, n_heads=HEADS, n_layers=LAYERS,\n",
    "        M=M, K=K, d_proj=D_PROJ, p=0.0\n",
    "    ).to(device)\n",
    "\n",
    "    # Router temperature: start softer, anneal later if desired\n",
    "    model.gater.tau = 1.5\n",
    "    model.lb_lambda = 0.0\n",
    "    model.ent_lambda = 0.0\n",
    "\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, betas=(0.9, 0.95), weight_decay=0.01)\n",
    "    crit = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    global_pos = 0\n",
    "    running = None\n",
    "\n",
    "    def finite_or_die(*tensors):\n",
    "        for t in tensors:\n",
    "            if not torch.isfinite(t).all():\n",
    "                raise RuntimeError(\"Non-finite detected. Lower LR, check mask, or print intermediates.\")\n",
    "\n",
    "    for step in range(STEPS):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        total_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        for _ in range(ACCUM):\n",
    "            T_total = 4 * CHUNK\n",
    "            x_full, y_full = make_stream_batch(BATCH, T_total, VOC, DELAY, device)\n",
    "            mem = model.init_mem(BATCH, device=device)\n",
    "\n",
    "            for s in range(0, T_total, CHUNK):\n",
    "                x = x_full[:, s:s+CHUNK]\n",
    "                y = y_full[:, s:s+CHUNK]\n",
    "\n",
    "                logits, mem, aux = model(x, mem, pos_offset=global_pos + s, topk=TOPK, collect_aux=True)\n",
    "                finite_or_die(logits, mem)\n",
    "\n",
    "                ce = crit(logits.reshape(-1, VOC), y.reshape(-1))\n",
    "\n",
    "                if step >= WARM_STEPS:\n",
    "                    # turn on tiny aux regularizers\n",
    "                    model.lb_lambda  = 1e-3\n",
    "                    model.ent_lambda = 5e-4\n",
    "                    lb  = aux.get(\"lb_loss\", ce.new_tensor(0.0))\n",
    "                    eb  = aux.get(\"ent_bonus\", ce.new_tensor(0.0))\n",
    "                else:\n",
    "                    lb = eb = ce.new_tensor(0.0)\n",
    "\n",
    "                loss_step = ce + lb + eb\n",
    "                finite_or_die(loss_step)\n",
    "                total_loss = total_loss + loss_step\n",
    "\n",
    "            global_pos += T_total\n",
    "\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = logits.argmax(-1)\n",
    "            mask = (y != -100)\n",
    "            acc = (pred[mask] == y[mask]).float().mean().item()\n",
    "            running = total_loss.item() if running is None else 0.95 * running + 0.05 * float(total_loss.item())\n",
    "\n",
    "        if (step + 1) % LOG_EVERY == 0:\n",
    "            print(f\"step {step+1:4d} | loss {running:.3f} | acc {acc:.3f} | gate_H {aux.get('gate_entropy', 0.0):.3f} | tau {model.gater.tau:.2f}\")\n",
    "\n",
    "        # (optional) very mild anneal of router temperature after warmup\n",
    "        if (step + 1) % 200 == 0 and step >= WARM_STEPS:\n",
    "            model.gater.tau = max(1.0, model.gater.tau * 0.95)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
