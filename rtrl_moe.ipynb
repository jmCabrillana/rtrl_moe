{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "couiBugqYvSM",
        "outputId": "99c01f70-5ce4-42cf-d305-d3125382bb63"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rtrl_moe'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 43 (delta 16), reused 35 (delta 8), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (43/43), 21.27 KiB | 5.32 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd rtrl_moe/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9A4st4Y-Wzgx",
        "outputId": "0d0904a6-dc00-4295-e449-e4d5240c1d8d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rtrl_moe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"jean_manuel.cabrillana@yahoo.fr\"\n",
        "!git config --global user.name \"jmCabrillana\""
      ],
      "metadata": {
        "id": "BE_FH4drckR_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrL92en_c6am",
        "outputId": "e9ae6afb-9935-4ee1-b23c-daf8a12f6b22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.func import jacrev, vmap, functional_call\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from einops import rearrange\n",
        "import random\n",
        "import time\n",
        "import importlib\n",
        "import re\n",
        "\n",
        "import structure\n",
        "from structure import CircularTree, sparse_left_mul\n",
        "from rtrl_minimal import RTRL\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SQvo6xuAsCUV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importlib.reload(structure)\n",
        "from structure import CircularTree, sparse_left_mul"
      ],
      "metadata": {
        "id": "RnUtlsA4JXrW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# per-point function to batch jacobian later\n",
        "def make_f_single(model, proj=slice(None)):\n",
        "    def f(params, h, x, kw):\n",
        "        x1, h1 = rearrange(x, '... -> 1 ...'), rearrange(h, '... -> 1 ...')\n",
        "        *_, h_next_b = functional_call(model, params, (x1, h1), kw)\n",
        "        return rearrange(h_next_b, '1 h -> h')[proj] # remove fake batch, slice\n",
        "    return f\n",
        "\n",
        "def add_grad_(p, g):\n",
        "    if g is None: return\n",
        "    g = g.detach()\n",
        "    if p.grad is None:\n",
        "        p.grad = g.clone()\n",
        "    else:\n",
        "        p.grad.add_(g)\n",
        "\n",
        "class BlockRTRL:\n",
        "    @torch.no_grad()\n",
        "    def __init__(self, state_params, B, H, len_buffer=64, len_buffer_hk=8):\n",
        "        self.state_params = state_params\n",
        "        self.P_t = {k:torch.zeros([B, H, p.numel()]).to(p) for k,p in self.state_params.items()}\n",
        "        self.last_update = {k:0 for k in self.state_params.keys()}\n",
        "        self.len_buffer = len_buffer\n",
        "        self.len_buffer_hk = len_buffer_hk\n",
        "        # self.buffer = CircularMatTree(len_buffer, torch.eye(H).expand(B, -1, -1).to(list(self.state_params.values())[0]))\n",
        "        self.buffer = CircularTree(len_buffer, None, sparse_left_mul)\n",
        "        self.last_hk = [0]\n",
        "        self.t = 0\n",
        "\n",
        "    def reset(self):\n",
        "        [P_t.zero_() for P_t in self.P_t.values()]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def get_left_product(self, k, l):\n",
        "        return self.buffer.query(k, l)\n",
        "      #   if l is None: l = len(self.buffer)\n",
        "      #   L = torch.eye(H).expand(B, -1, -1)\n",
        "      #   for i in range(k, l):\n",
        "      #     L = torch.bmm(self.buffer[i], L)\n",
        "      #   return L\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, model, x_t, h_t, loss, active_params, proj=None, **kw):\n",
        "        \"\"\"\n",
        "        x_t: [B,...], h_t: [B,H], P_t: [B,H,Tp], dL_dH_t: [B,H]\n",
        "        \"\"\"\n",
        "        params = dict(model.named_parameters())\n",
        "        B, H = h_t.shape[:2]\n",
        "        proj = list(range(H)) if proj is None else proj\n",
        "        f1 = make_f_single(model, proj)\n",
        "\n",
        "        # batched jacobian of per-sample f\n",
        "        Jh_proj = vmap(jacrev(f1, argnums=1), in_dims=(None, 0, 0, None))(active_params, h_t, x_t, kw)  # [B,H,H]\n",
        "        # Jh = torch.eye(H).repeat(B, 1, 1).to(h_t), Jh[:, proj] = Jh_proj\n",
        "        Jtheta_proj = vmap(jacrev(f1, argnums=0), in_dims=(None, 0, 0, None))(active_params, h_t, x_t, kw) # [B,H,[...]]\n",
        "        Jtheta_proj = {k:rearrange(v, 'b h ... -> b h (...)') for k, v in Jtheta_proj.items()}  # [B,H,[Tp]]\n",
        "        # >>> Detach before storing <<<\n",
        "        Jh_proj = Jh_proj.detach()\n",
        "        Jtheta_proj = {k: v.detach() for k, v in Jtheta_proj.items()}\n",
        "\n",
        "        # Update circular buffer\n",
        "        self.buffer.update((proj, Jh_proj))\n",
        "        # self.buffer.append(Jh); if len(self.buffer) > self.len_buffer: self.buffer.pop(0)\n",
        "\n",
        "        # RTRL recursion on active or expiring sensitivities\n",
        "        for k in self.state_params.keys():\n",
        "            # Active parameters update\n",
        "            if k in active_params.keys():\n",
        "                # Jtheta = torch.zeros([B, H, self.state_params[k].numel()]); Jtheta[:, proj] = Jtheta_tree[k]\n",
        "                # t <-> index [q-1]; product for time s < t starts at s+1, ie index [s - t + (q-1) + 1] = [s - t + q]\n",
        "                idx, L_Jh = self.get_left_product(self.last_update[k] - self.t + self.len_buffer, self.len_buffer)\n",
        "                self.P_t[k][:, idx] = L_Jh @ self.P_t[k]\n",
        "                self.P_t[k][:, proj] += Jtheta_proj[k]\n",
        "                self.last_update[k] = self.t\n",
        "            # Expiring parameters update\n",
        "            idx, L_Jh = self.get_left_product(0, self.len_buffer)\n",
        "            if k not in active_params.keys() and self.last_update[k] <= self.t - self.len_buffer: # == in practice\n",
        "                self.P_t[k][:, idx] = L_Jh @ self.P_t[k]\n",
        "                self.last_update[k] = self.t\n",
        "\n",
        "\n",
        "        # Backpropagate, Maintain last activated\n",
        "        if loss is not None:\n",
        "            dL_dTheta = dict(zip(params.keys(), torch.autograd.grad(loss, params.values(), retain_graph=True, allow_unused=True)))\n",
        "            (dL_dH_t,) = torch.autograd.grad(loss, h_t, retain_graph=False) #[:,self.last_hk]\n",
        "            for k, g in dL_dTheta.items():\n",
        "                add_grad_(params[k], g)\n",
        "            for k, p in self.state_params.items():\n",
        "                g_mean = torch.einsum('b h, b h t -> b t', dL_dH_t, self.P_t[k]).mean(0) #[self.last_hk]\n",
        "                add_grad_(params[k], g_mean.view(p.shape))\n",
        "        if len(proj) != H:\n",
        "            I = set(self.last_hk) & set(proj)\n",
        "            self.last_hk = ([k for k in self.last_hk + proj if k not in I] + list(I))\n",
        "            self.last_hk = self.last_hk[-self.len_buffer_hk:]\n",
        "\n",
        "        self.t += 1\n"
      ],
      "metadata": {
        "id": "tgr-WX499Vt5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class Dummy(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=3, output_dim=3):\n",
        "        super().__init__()\n",
        "        self.state_linear = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
        "        self.output_linear = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        # concatenate input and state\n",
        "        z = torch.cat([x, h], dim=-1)\n",
        "        h_next = torch.tanh(self.state_linear(z))\n",
        "        y = self.output_linear(h_next)\n",
        "        return y, h_next\n",
        "\n",
        "class Dummy2(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=3):\n",
        "        super().__init__()\n",
        "        self.state_fc1 = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
        "        self.state_fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.state_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.output_fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output_fc2 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        z = torch.cat([x, h], dim=-1)\n",
        "        s = F.silu(self.state_fc1(z))\n",
        "        s = s + F.silu(self.state_fc2(s))\n",
        "        s = self.state_norm(s)\n",
        "        h_next = torch.tanh(s)\n",
        "        y = self.output_fc2(F.silu(self.output_fc1(h_next)))\n",
        "        return y, h_next\n",
        "\n",
        "\n",
        "model = Dummy(input_dim=5, hidden_dim=3, output_dim=5).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "B, H, D = 1, 3, 5\n",
        "state_params = {k:v.to(device) for k,v in model.named_parameters() if k.startswith(\"state_\")}\n",
        "rtrl = BlockRTRL(state_params, B, H)\n",
        "# writer = SummaryWriter(log_dir=\"runs/block_rtrl_1\")\n",
        "t=0"
      ],
      "metadata": {
        "id": "gfvTD9_fQZDN",
        "cellView": "form"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "n=10\n",
        "for _ in range(n):\n",
        "  x_t = torch.randn(B, D).to(device)   # input batch\n",
        "  h_t = torch.randn(B, H).to(device).requires_grad_()   # hidden states\n",
        "  target = 2*x_t + 0.5\n",
        "  y, h_next = model(x_t, h_t)\n",
        "  loss = ((y - target) ** 2).mean()\n",
        "  optimizer.zero_grad()\n",
        "  rtrl.step(model, x_t, h_t, loss, state_params, [t % H])\n",
        "  # loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "  optimizer.step()\n",
        "  h_t = h_next.detach()\n",
        "  # writer.add_scalar(\"train/loss\", loss.item(), t)\n",
        "  print(loss.item())\n",
        "  t += 1"
      ],
      "metadata": {
        "id": "it6k4vKpm90y",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# use same Dummy as you defined above\n",
        "B, H, D = 1, 30, 2\n",
        "model = Dummy2(input_dim=D, hidden_dim=H).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "state_params = {k:v.to(device) for k,v in model.named_parameters() if k.startswith(\"state_\")}\n",
        "rtrl = BlockRTRL(state_params, B, H)\n",
        "writer = SummaryWriter(log_dir=\"runs/anbn_Brtrl_0\")\n",
        "t = 0\n",
        "\n",
        "def make_seq(len_max_a=8):\n",
        "    # half the time make a valid a^n b^n, otherwise pick a mismatched n_b\n",
        "    n_a = random.randint(1, len_max_a)\n",
        "    if random.random() < 0.5:\n",
        "        n_b = n_a\n",
        "        tgt = torch.tensor([[1.0]]).to(device)\n",
        "    else:\n",
        "        n_b = random.choice([k for k in range(1,len_max_a+1) if k != n_a])\n",
        "        tgt = torch.tensor([[0.0]]).to(device)\n",
        "    return tgt, torch.cat([torch.tensor([[1.,0.]]).repeat(n_a,1), torch.tensor([[0.,1.]]).repeat(n_b,1)], dim=0).to(device)"
      ],
      "metadata": {
        "id": "fCCfYfAOjjOs",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "n_steps = 100\n",
        "for step in range(n_steps):\n",
        "  tgt, x_seq = make_seq()\n",
        "  h_t = torch.zeros(1, H).requires_grad_().to(device)\n",
        "  rtrl.reset()\n",
        "  for k in range(x_seq.size(0)):\n",
        "      y, h_next = model(x_seq[k].unsqueeze(0), h_t)\n",
        "      if k < x_seq.size(0) - 2:\n",
        "          rtrl.step(model, x_seq[k].unsqueeze(0), h_t, None, active_params, [t % H, (t+1) % H])\n",
        "          h_t = h_next.detach()\n",
        "      if k == x_seq.size(0) - 2:\n",
        "          h_t = h_next.detach().requires_grad_()\n",
        "\n",
        "  loss = ((y - tgt) ** 2).mean()\n",
        "  optimizer.zero_grad()\n",
        "  # loss.backward()\n",
        "  rtrl.step(model, x_seq[k].unsqueeze(0), h_t, loss, active_params, [t % H, (t+1) % H])\n",
        "  nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  optimizer.step()\n",
        "  writer.add_scalar(\"train/loss\", loss.item(), t)\n",
        "  writer.add_scalar(\"train/P_t\", list(rtrl.P_t.values())[0].abs().mean().item(), t)\n",
        "  t += 1"
      ],
      "metadata": {
        "id": "QKz0kSE9owtw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "sDa21MvISpJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MoE"
      ],
      "metadata": {
        "id": "3DIZMAun_0C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class ExpertBank(nn.Module):\n",
        "    \"\"\"\n",
        "    E experts, each: y = act(W[e] @ x + b[e])\n",
        "    Params are stored per-expert (ParameterList) but used via stacked banks in forward.\n",
        "    \"\"\"\n",
        "    def __init__(self, E, d):\n",
        "        super().__init__()\n",
        "        self.E, self.d = E, d\n",
        "        self.W = nn.ParameterList([nn.Parameter(torch.empty(d, d)) for _ in range(E)])\n",
        "        self.b = nn.ParameterList([nn.Parameter(torch.empty(d))    for _ in range(E)])\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for W, b in zip(self.W, self.b):\n",
        "            nn.init.kaiming_uniform_(W, a=5**0.5)  # good for ReLU\n",
        "            fan_in = W.size(1)\n",
        "            bound = 1.0 / fan_in**0.5\n",
        "            nn.init.uniform_(b, -bound, bound)\n",
        "\n",
        "    def forward(self, x, w, idx):\n",
        "        B, T, D = x.shape\n",
        "        _, k = idx.shape\n",
        "\n",
        "        W_bank = torch.stack(list(self.W), dim=0).contiguous()  # [E, D, D]\n",
        "        b_bank = torch.stack(list(self.b), dim=0).contiguous()  # [E, D]\n",
        "\n",
        "        flat_idx = idx.reshape(-1)\n",
        "        W_sel = W_bank.index_select(0, flat_idx).reshape(B, k, D, D).contiguous()   # [B,k,D,D]\n",
        "        b_sel = b_bank.index_select(0, flat_idx).reshape(B, k, D).contiguous()      # [B,k,D]\n",
        "\n",
        "        y_k = torch.einsum('b t i, b k i o -> b t k o', x, W_sel) + b_sel.unsqueeze(1)\n",
        "        y_k = F.relu(y_k)\n",
        "        y = (y_k * w.view(B,1,k,1)).sum(dim=2)  # [B,T,D]\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "XNUnecbbnweY",
        "cellView": "form"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, d, mult=1, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d, mult * d)\n",
        "        self.fc2 = nn.Linear(mult * d, d)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(x))))\n",
        "\n",
        "class TopKGate(nn.Module):\n",
        "    def __init__(self, d, n_experts, k=2):\n",
        "        super().__init__()\n",
        "        assert 1 <= k <= n_experts\n",
        "        self.k, self.n = k, n_experts\n",
        "        self.proj = nn.Linear(d, n_experts, bias=False)\n",
        "    def forward(self, h):                         # h: [B,D]\n",
        "        logits = self.proj(h)                     # [B,E]\n",
        "        val, idx = torch.topk(logits, self.k, dim=-1)  # [B,k]\n",
        "        return val.softmax(-1), idx               # normalize over top-k\n",
        "\n",
        "# ---------- Recurrent MoE ----------\n",
        "class RecurrentMoE(nn.Module):\n",
        "    \"\"\"\n",
        "    Step:\n",
        "      1) Mixed-attn: Q=latent, K/V=concat(latent, x)\n",
        "      2) Pool latent -> gate -> mixture of experts (sequence-level)\n",
        "      3) Write expert update back to latent slots chosen by a small gate\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=512, n_heads=2, n_slots=4, n_experts=4, topk=2, dropout=0.0):\n",
        "        super().__init__()\n",
        "        d, E = d_model, n_experts\n",
        "        self.d, self.n_slots = d, n_slots\n",
        "\n",
        "        # Attention\n",
        "        self.state_mha = nn.MultiheadAttention(d, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.state_ln_q = nn.LayerNorm(d)\n",
        "        self.state_ln_kv = nn.LayerNorm(d)\n",
        "        self.state_ffn = MLP(d, dropout=dropout)\n",
        "        self.state_ln_ffn = nn.LayerNorm(d)\n",
        "\n",
        "        # Sequence level gating with topk expert\n",
        "        self.state_gate = TopKGate(d, n_experts, k=topk)\n",
        "        # self.state_experts = nn.ModuleList([MLP(d, dropout=dropout) for _ in range(n_experts)])\n",
        "        self.state_experts = ExpertBank(E, d)\n",
        "        self.state_ln_moe_in = nn.LayerNorm(d)\n",
        "        self.state_latent_proj = nn.Linear(d, d)\n",
        "\n",
        "        # Latent projection\n",
        "        self.state_ln_slot = nn.LayerNorm(d)\n",
        "        self.state_slot_ctx = nn.Linear(d, 1, bias=False)\n",
        "        self.out = nn.Linear(n_slots*d, d)\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def init_state(self, batch_size, device=None, dtype=None):\n",
        "        device = device or next(self.parameters()).device\n",
        "        dtype = dtype or next(self.parameters()).dtype\n",
        "        latent = torch.randn(batch_size, self.n_slots*self.d, device=device, dtype=dtype) * 0.02\n",
        "        return latent\n",
        "\n",
        "    def forward(self, x, state_flat):\n",
        "        B, S, D = state_flat.shape[0], self.n_slots, self.d\n",
        "        device, dtype = state_flat.device, state_flat.dtype\n",
        "        state = state_flat.reshape([B, S, D]).contiguous()\n",
        "        latent = state.clone()\n",
        "\n",
        "        # (1) Mixed attention\n",
        "        q = self.state_ln_q(latent)\n",
        "        kv = torch.cat([latent, x], dim=1)\n",
        "        kv = self.state_ln_kv(kv)\n",
        "        attn_out, _ = self.state_mha(q, kv, kv, need_weights=False)\n",
        "        latent = latent + attn_out\n",
        "        latent = latent + self.state_ffn(self.state_ln_ffn(latent))\n",
        "\n",
        "        # (2) MoE with top-k experts (sequence-level) =1 here\n",
        "        pooled = self.state_ln_moe_in(latent.mean(dim=1))       # [B,D]\n",
        "        w, idx = self.state_gate(pooled)                        # [B,k]]\n",
        "        # mixed = torch.zeros(B, S, D, device=latent.device, dtype=latent.dtype)\n",
        "        # for i in range(self.state_gate.k):\n",
        "        #     for e in range(len(self.state_experts)):\n",
        "        #         mask = (idx[:, i] == e)                   # [B]\n",
        "        #         if mask.any():\n",
        "        #             acc = torch.einsum('b, b S D-> b S D', w[mask, i], self.state_experts[e](latent[mask]))\n",
        "        #             mixed = mixed.index_put((mask,), acc, accumulate=True)\n",
        "        mixed = self.state_experts(latent, w, idx)\n",
        "        latent = latent + mixed\n",
        "\n",
        "        # (3) Choose *one* target slot per sample and update state only there\n",
        "        logits = self.state_slot_ctx(self.state_ln_slot(latent)).squeeze(-1)        # [B,S]\n",
        "        w, tgt_idx = torch.topk(logits, 2, dim=-1)                          # [B,2]\n",
        "        # for i in range(2):\n",
        "        #     for s in range(S):\n",
        "        #         mask = (tgt_idx[:, i] == s)\n",
        "        #         acc = torch.einsum('b, b D-> b D', w[mask, i], latent[mask, s])\n",
        "        #         state = state.index_put((mask, torch.tensor(s, device=device)),\n",
        "        #                                 acc, accumulate=True)\n",
        "        alpha = torch.zeros(B, S, device=device, dtype=dtype)\n",
        "        alpha = alpha.scatter_add_(1, tgt_idx, w)\n",
        "        state = state + alpha.unsqueeze(-1) * latent     # out-of-place residual update\n",
        "\n",
        "        # Output and info\n",
        "        y = self.out(latent.reshape([B, S*D]))\n",
        "        info = {\n",
        "            \"idx_experts\": idx.detach(),\n",
        "            \"idx_slots\": tgt_idx.detach()\n",
        "        }\n",
        "        return y, info, state.reshape([B, S*D]).contiguous()\n",
        "\n",
        "# ---------- Tiny usage ----------\n",
        "B, T, D = 1, 8, 64\n",
        "model = RecurrentMoE(d_model=D, n_heads=4, n_slots=64, n_experts=64, topk=2).to(device)\n",
        "x = torch.randn(B, T, D).to(device)\n",
        "pattern = re.compile(r'^state_experts\\.(W|b)\\.\\d+$')\n",
        "state_params = {name:p for name,p in model.named_parameters() if name.startswith(\"state_\")}\n",
        "core_params = {name:p for name, p in state_params.items() if pattern.match(name) is None}\n",
        "expert_params = {name:p for name, p in state_params.items() if pattern.match(name)}\n",
        "state = model.init_state(B, device=x.device).requires_grad_()\n",
        "rtrl = BlockRTRL(state_params, B, state.shape[-1])"
      ],
      "metadata": {
        "id": "21_HUyBWxpuk"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y, info, state_new = model(x, state)\n",
        "loss = ((y - torch.zeros_like(y))**2).mean()"
      ],
      "metadata": {
        "id": "2Oge1lgX2UNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_expert_latent_activated(info):\n",
        "    idx_slots = list(set(info['idx_slots'].flatten().tolist()))\n",
        "    proj = sum((list(range(D*i, D*(i+1))) for i in idx_slots), start=[])\n",
        "    expert_ids = list(set(info['idx_experts'].flatten().tolist()))\n",
        "    ids_pattern = \"|\".join(map(str, expert_ids))\n",
        "    pattern = re.compile(rf'^state_experts\\.(W|b)\\.({ids_pattern})$')\n",
        "    active_experts_params = {name:p for name, p in state_params.items() if pattern.match(name)}\n",
        "    active_params = core_params | active_experts_params\n",
        "    return active_params, proj\n",
        "active_params, proj = get_expert_latent_activated(info)"
      ],
      "metadata": {
        "id": "uDkFm4AElPXq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rtrl.step(model, x, state, loss, active_params, proj)\n",
        "state = state_new.detach()"
      ],
      "metadata": {
        "id": "4HjyqKjQeCre",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff60b25-0f20-44fb-b685-a130ad30dfe1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:829: UserWarning: There is a performance drop because we have not yet implemented the batching rule for aten::_scaled_dot_product_efficient_attention_backward. Please file us an issue on GitHub so that we can prioritize its implementation. (Triggered internally at /pytorch/aten/src/ATen/functorch/BatchedFallback.cpp:81.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p[1].numel() for p in rtrl.buffer.tree if p is not None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivps-vE9siFp",
        "outputId": "e3e53f30-eb6e-4eb9-c0cf-be615a2aaab4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "122683392"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(p.numel() for p in rtrl.P_t.values())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-e7FhLe5Ee-",
        "outputId": "a1d67257-e500-4805-9b4f-7b62b3ae2a69"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21004288"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(rtrl.P_t.values())[2].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmUcB_tExNOk",
        "outputId": "1ab3982f-8a14-4444-dc30-a128db5c8cf9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 8192, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN    = 5000\n",
        "DELAY_MIN  = 50\n",
        "DELAY_MAX  = 200\n",
        "TBPTT      = 32\n",
        "H          = 128\n",
        "D          = 16  # vocab\n",
        "\n",
        "# --- Data: random tokens and per-step random delay ---\n",
        "x = torch.randint(0, D, (SEQ_LEN,), dtype=torch.long)\n",
        "d = torch.randint(DELAY_MIN, DELAY_MAX + 1, (SEQ_LEN,), dtype=torch.long)\n",
        "\n",
        "# y_t = x_{t - d_t} if t - d_t >= 0 else ignore\n",
        "y = torch.full((SEQ_LEN,), -100, dtype=torch.long)  # -100 = ignore_index\n",
        "valid = torch.arange(SEQ_LEN) - d >= 0\n",
        "idx = torch.nonzero(valid, as_tuple=False).squeeze(-1)\n",
        "y[idx] = x[idx - d[idx]]\n",
        "x, y = x.to(device), y.to(device)\n",
        "\n",
        "# model = Dummy(input_dim=D, hidden_dim=H, output_dim=D).to(device)\n",
        "model = RecurrentMoE(d_model=D, n_heads=4, n_slots=64, n_experts=64, topk=2).to(device)\n",
        "pattern = re.compile(r'^state_experts\\.(W|b)\\.\\d+$')\n",
        "state_params = {name:p for name,p in model.named_parameters() if name.startswith(\"state_\")}\n",
        "core_params = {name:p for name, p in state_params.items() if pattern.match(name) is None}\n",
        "expert_params = {name:p for name, p in state_params.items() if pattern.match(name)}\n",
        "\n",
        "h = model.init_state(1, device=device).requires_grad_()\n",
        "rtrl = BlockRTRL(state_params, B, h.shape[-1])\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100, reduction=\"none\")  # per-step"
      ],
      "metadata": {
        "id": "uyyXP1VRAUHa"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training: chunked outer loop, per-step inner loop; backward every step ---\n",
        "model.train()\n",
        "running = 0.0\n",
        "for epoch in range(3):\n",
        "    # h = torch.zeros(1, H, device=device)\n",
        "    h = model.init_state(1, device=device).requires_grad_()\n",
        "    for start in range(0, SEQ_LEN, TBPTT):\n",
        "        end = min(start + TBPTT, SEQ_LEN)\n",
        "\n",
        "        # one-hot inputs for this chunk\n",
        "        x_chunk = F.one_hot(x[start:end], num_classes=D).float()  # [T, V]\n",
        "        y_chunk = y[start:end]  # [T]\n",
        "        loss = torch.zeros(1, device=device)\n",
        "\n",
        "        for t in range(x_chunk.size(0)):\n",
        "            xt = x_chunk[t][None, None, ...]        # [1, 1, D]\n",
        "            yt = y_chunk[t].unsqueeze(0)            # [1]\n",
        "\n",
        "            opt.zero_grad()\n",
        "            logits, info, h_new = model(xt, h)             # logits: [1, D]\n",
        "            active_params, proj = get_expert_latent_activated(info)\n",
        "            loss_t = criterion(logits, yt).mean()  # scalar\n",
        "\n",
        "            # Only backprop if target is valid; always advance state\n",
        "            if yt.item() != -100:\n",
        "                # rtrl.step(model, xt, h, loss_t, active_params, proj)\n",
        "                # opt.step()\n",
        "                loss = loss + loss_t\n",
        "                running = (0.98 * running + 0.02 * loss_t.item()) if running else loss_t.item()\n",
        "            else:\n",
        "                # rtrl.step(model, xt, h, None, active_params, proj)\n",
        "                pass\n",
        "            h = h_new #.detach().requires_grad_()\n",
        "            print(f\"{running:.4f}\")\n",
        "        loss_t.backward()\n",
        "        h = h.detach()\n",
        "        opt.step()\n",
        "\n",
        "    print(f\"epoch {epoch+1} | running CE (masked): {running:.4f}\")\n",
        "\n",
        "# --- Eval on same stream (for simplicity) ---\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     h = torch.zeros(1, H, device=device)\n",
        "#     preds = []\n",
        "#     for t in range(SEQ_LEN):\n",
        "#         xt = F.one_hot(x[t], num_classes=D).float().unsqueeze(0)\n",
        "#         logits, h = model(xt, h)\n",
        "#         preds.append(logits.argmax(dim=-1).item())\n",
        "#     preds = torch.tensor(preds, device=device)\n",
        "\n",
        "# mask = (y != -100)\n",
        "# acc = (preds[mask] == y[mask]).float().mean().item()\n",
        "# print(f\"masked per-step accuracy: {acc*100:.2f}% \"\n",
        "#       f\"(TBPTT={TBPTT}, delayâˆˆ[{DELAY_MIN},{DELAY_MAX}], H={H})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SC72RjUJxzCt",
        "outputId": "bbedbd8d-cfd3-49ee-cfe7-23ea4678ea7d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "0.0000\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5287\n",
            "2.5347\n",
            "2.5347\n",
            "2.5407\n",
            "2.5407\n",
            "2.5407\n",
            "2.5407\n",
            "2.5407\n",
            "2.5407\n",
            "2.5494\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5549\n",
            "2.5550\n",
            "2.5611\n",
            "2.5611\n",
            "2.5611\n",
            "2.5695\n",
            "2.5695\n",
            "2.5730\n",
            "2.5737\n",
            "2.5731\n",
            "2.5731\n",
            "2.5731\n",
            "2.5731\n",
            "2.5790\n",
            "2.5831\n",
            "2.5831\n",
            "2.5894\n",
            "2.5894\n",
            "2.5894\n",
            "2.5925\n",
            "2.5982\n",
            "2.5982\n",
            "2.6026\n",
            "2.6026\n",
            "2.6026\n",
            "2.6064\n",
            "2.6064\n",
            "2.6108\n",
            "2.6166\n",
            "2.6166\n",
            "2.6166\n",
            "2.6194\n",
            "2.6245\n",
            "2.6313\n",
            "2.6364\n",
            "2.6364\n",
            "2.6411\n",
            "2.6482\n",
            "2.6482\n",
            "2.6495\n",
            "2.6495\n",
            "2.6495\n",
            "2.6539\n",
            "2.6606\n",
            "2.6640\n",
            "2.6640\n",
            "2.6683\n",
            "2.6749\n",
            "2.6768\n",
            "2.6757\n",
            "2.6735\n",
            "2.6777\n",
            "2.6754\n",
            "2.6786\n",
            "2.6827\n",
            "2.6827\n",
            "2.6878\n",
            "2.6929\n",
            "2.6957\n",
            "2.6972\n",
            "2.7025\n",
            "2.7069\n",
            "2.7117\n",
            "2.7048\n",
            "2.7078\n",
            "2.7145\n",
            "2.7175\n",
            "2.7213\n",
            "2.7258\n",
            "2.7295\n",
            "2.7220\n",
            "2.7283\n",
            "2.7286\n",
            "2.7331\n",
            "2.7368\n",
            "2.7294\n",
            "2.7299\n",
            "2.7299\n",
            "2.7224\n",
            "2.7262\n",
            "2.7319\n",
            "2.7372\n",
            "2.7407\n",
            "2.7433\n",
            "2.7488\n",
            "2.7511\n",
            "2.7493\n",
            "2.7531\n",
            "2.7552\n",
            "2.7521\n",
            "2.7492\n",
            "2.7515\n",
            "2.7569\n",
            "2.7577\n",
            "2.7468\n",
            "2.7518\n",
            "2.7578\n",
            "2.7547\n",
            "2.7601\n",
            "2.7608\n",
            "2.7661\n",
            "2.7586\n",
            "2.7515\n",
            "2.7496\n",
            "2.7504\n",
            "2.7500\n",
            "2.7470\n",
            "2.7513\n",
            "2.7520\n",
            "2.7563\n",
            "2.7615\n",
            "2.7621\n",
            "2.7508\n",
            "2.7402\n",
            "2.7398\n",
            "2.7443\n",
            "2.7414\n",
            "2.7474\n",
            "2.7528\n",
            "2.7583\n",
            "2.7469\n",
            "2.7358\n",
            "2.7430\n",
            "2.7473\n",
            "2.7493\n",
            "2.7545\n",
            "2.7609\n",
            "2.7602\n",
            "2.7619\n",
            "2.7659\n",
            "2.7662\n",
            "2.7679\n",
            "2.7681\n",
            "2.7741\n",
            "2.7770\n",
            "2.7796\n",
            "2.7851\n",
            "2.7848\n",
            "2.7902\n",
            "2.7898\n",
            "2.7926\n",
            "2.7870\n",
            "2.7897\n",
            "2.7909\n",
            "2.7895\n",
            "2.7893\n",
            "2.7973\n",
            "2.7836\n",
            "2.7814\n",
            "2.7848\n",
            "2.7848\n",
            "2.7878\n",
            "2.7933\n",
            "2.7969\n",
            "2.7944\n",
            "2.7886\n",
            "2.7949\n",
            "2.8017\n",
            "2.7898\n",
            "2.7926\n",
            "2.7963\n",
            "2.7930\n",
            "2.7955\n",
            "2.8019\n",
            "2.8056\n",
            "2.8065\n",
            "2.7909\n",
            "2.7935\n",
            "2.8027\n",
            "2.8036\n",
            "2.8099\n",
            "2.8065\n",
            "2.8127\n",
            "2.8188\n",
            "2.8249\n",
            "2.8302\n",
            "2.8379\n",
            "2.8336\n",
            "2.8333\n",
            "2.8408\n",
            "2.8368\n",
            "2.8371\n",
            "2.8329\n",
            "2.8347\n",
            "2.8432\n",
            "2.8448\n",
            "2.8282\n",
            "2.8343\n",
            "2.8420\n",
            "2.8452\n",
            "2.8525\n",
            "2.8535\n",
            "2.8565\n",
            "2.8635\n",
            "2.8684\n",
            "2.8678\n",
            "2.8702\n",
            "2.8740\n",
            "2.8564\n",
            "2.8562\n",
            "2.8631\n",
            "2.8560\n",
            "2.8603\n",
            "2.8677\n",
            "2.8729\n",
            "2.8798\n",
            "2.8855\n",
            "2.8716\n",
            "2.8815\n",
            "2.8676\n",
            "2.8763\n",
            "2.8781\n",
            "2.8774\n",
            "2.8632\n",
            "2.8669\n",
            "2.8717\n",
            "2.8814\n",
            "2.8815\n",
            "2.8642\n",
            "2.8502\n",
            "2.8620\n",
            "2.8738\n",
            "2.8562\n",
            "2.8563\n",
            "2.8616\n",
            "2.8696\n",
            "2.8708\n",
            "2.8703\n",
            "2.8781\n",
            "2.8803\n",
            "2.8821\n",
            "2.8896\n",
            "2.8913\n",
            "2.8904\n",
            "2.9005\n",
            "2.9020\n",
            "2.8918\n",
            "2.9023\n",
            "2.9016\n",
            "2.9111\n",
            "2.9128\n",
            "2.9174\n",
            "2.9267\n",
            "2.9309\n",
            "2.9123\n",
            "2.9111\n",
            "2.8956\n",
            "2.9067\n",
            "2.9057\n",
            "2.9063\n",
            "2.8962\n",
            "2.8979\n",
            "2.9006\n",
            "2.9086\n",
            "2.9164\n",
            "2.9044\n",
            "2.9173\n",
            "2.9234\n",
            "2.9203\n",
            "2.9266\n",
            "2.9330\n",
            "2.9319\n",
            "2.9126\n",
            "2.9212\n",
            "2.9300\n",
            "2.9369\n",
            "2.9431\n",
            "2.9417\n",
            "2.9478\n",
            "2.9540\n",
            "2.9500\n",
            "2.9618\n",
            "2.9576\n",
            "2.9682\n",
            "2.9797\n",
            "2.9821\n",
            "2.9930\n",
            "2.9978\n",
            "2.9927\n",
            "2.9905\n",
            "3.0015\n",
            "3.0072\n",
            "3.0184\n",
            "3.0045\n",
            "2.9982\n",
            "3.0054\n",
            "3.0153\n",
            "3.0180\n",
            "3.0207\n",
            "3.0139\n",
            "3.0238\n",
            "3.0365\n",
            "3.0487\n",
            "3.0325\n",
            "3.0406\n",
            "3.0410\n",
            "3.0251\n",
            "3.0374\n",
            "3.0494\n",
            "3.0343\n",
            "3.0404\n",
            "3.0435\n",
            "3.0547\n",
            "3.0598\n",
            "3.0515\n",
            "3.0519\n",
            "3.0429\n",
            "3.0473\n",
            "3.0354\n",
            "3.0446\n",
            "3.0530\n",
            "3.0436\n",
            "3.0521\n",
            "3.0555\n",
            "3.0662\n",
            "3.0568\n",
            "3.0668\n",
            "3.0750\n",
            "3.0795\n",
            "3.0901\n",
            "3.0957\n",
            "3.0739\n",
            "3.0854\n",
            "3.0957\n",
            "3.0996\n",
            "3.1088\n",
            "3.1144\n",
            "3.1189\n",
            "3.1039\n",
            "3.1048\n",
            "3.1106\n",
            "3.1031\n",
            "3.0887\n",
            "3.0801\n",
            "3.0801\n",
            "3.0913\n",
            "3.0985\n",
            "3.0981\n",
            "3.0990\n",
            "3.0994\n",
            "3.0872\n",
            "3.0740\n",
            "3.0825\n",
            "3.0908\n",
            "3.0895\n",
            "3.0873\n",
            "3.0802\n",
            "3.0798\n",
            "3.0835\n",
            "3.0862\n",
            "3.0962\n",
            "3.0973\n",
            "3.0826\n",
            "3.0907\n",
            "3.0917\n",
            "3.0869\n",
            "3.0759\n",
            "3.0810\n",
            "3.0787\n",
            "3.0733\n",
            "3.0715\n",
            "3.0699\n",
            "3.0742\n",
            "3.0810\n",
            "3.0690\n",
            "3.0739\n",
            "3.0568\n",
            "3.0465\n",
            "3.0469\n",
            "3.0384\n",
            "3.0453\n",
            "3.0448\n",
            "3.0438\n",
            "3.0377\n",
            "3.0286\n",
            "3.0180\n",
            "3.0183\n",
            "3.0080\n",
            "2.9919\n",
            "2.9958\n",
            "3.0035\n",
            "3.0040\n",
            "2.9943\n",
            "2.9900\n",
            "2.9973\n",
            "2.9862\n",
            "2.9880\n",
            "2.9786\n",
            "2.9867\n",
            "2.9872\n",
            "2.9826\n",
            "2.9832\n",
            "2.9832\n",
            "2.9726\n",
            "2.9622\n",
            "2.9573\n",
            "2.9530\n",
            "2.9460\n",
            "2.9549\n",
            "2.9637\n",
            "2.9541\n",
            "2.9622\n",
            "2.9642\n",
            "2.9554\n",
            "2.9485\n",
            "2.9516\n",
            "2.9532\n",
            "2.9545\n",
            "2.9451\n",
            "2.9314\n",
            "2.9278\n",
            "2.9249\n",
            "2.9179\n",
            "2.9212\n",
            "2.9310\n",
            "2.9404\n",
            "2.9398\n",
            "2.9328\n",
            "2.9355\n",
            "2.9289\n",
            "2.9168\n",
            "2.9207\n",
            "2.9114\n",
            "2.9023\n",
            "2.8999\n",
            "2.8941\n",
            "2.8977\n",
            "2.9011\n",
            "2.8947\n",
            "2.8976\n",
            "2.8885\n",
            "2.8916\n",
            "2.8960\n",
            "2.8903\n",
            "2.8848\n",
            "2.8828\n",
            "2.8933\n",
            "2.8938\n",
            "2.8825\n",
            "2.8866\n",
            "2.8905\n",
            "2.9006\n",
            "2.9107\n",
            "2.8994\n",
            "2.8897\n",
            "2.8818\n",
            "2.8746\n",
            "2.8648\n",
            "2.8712\n",
            "2.8778\n",
            "2.8814\n",
            "2.8828\n",
            "2.8903\n",
            "2.8976\n",
            "2.8918\n",
            "2.8851\n",
            "2.8913\n",
            "2.8877\n",
            "2.8815\n",
            "2.8802\n",
            "2.8731\n",
            "2.8708\n",
            "2.8677\n",
            "2.8577\n",
            "2.8568\n",
            "2.8634\n",
            "2.8590\n",
            "2.8550\n",
            "2.8598\n",
            "2.8697\n",
            "2.8742\n",
            "2.8787\n",
            "2.8860\n",
            "2.8928\n",
            "2.8864\n",
            "2.8714\n",
            "2.8824\n",
            "2.8777\n",
            "2.8836\n",
            "2.8941\n",
            "2.8923\n",
            "2.9617\n",
            "2.9827\n",
            "3.0135\n",
            "3.0376\n",
            "3.0737\n",
            "3.1256\n",
            "3.1635\n",
            "3.1988\n",
            "3.2405\n",
            "3.3325\n",
            "3.3064\n",
            "3.4020\n",
            "3.4621\n",
            "3.7483\n",
            "4.1324\n",
            "4.1802\n",
            "4.5880\n",
            "4.4962\n",
            "4.4063\n",
            "5.3921\n",
            "6.1333\n",
            "6.9991\n",
            "8.4460\n",
            "9.0739\n",
            "9.7284\n",
            "13.4053\n",
            "15.8701\n",
            "18.9183\n",
            "19.2861\n",
            "35.1100\n",
            "52.8750\n",
            "72.3838\n",
            "196.3502\n",
            "268.0787\n",
            "615.9060\n",
            "793.5758\n",
            "1547.0889\n",
            "2383.5961\n",
            "3169.5073\n",
            "3907.7036\n",
            "4162.7065\n",
            "4433.8902\n",
            "4976.1811\n",
            "4876.6574\n",
            "5454.6999\n",
            "5747.8233\n",
            "6732.2500\n",
            "7707.2038\n",
            "8675.0305\n",
            "9791.0389\n",
            "10900.2438\n",
            "11819.2860\n",
            "11582.9003\n",
            "12116.7845\n",
            "13011.0891\n",
            "14078.3226\n",
            "14551.9979\n",
            "15574.0684\n",
            "16515.9100\n",
            "17457.1738\n",
            "17788.5872\n",
            "17432.8155\n",
            "18107.8551\n",
            "18397.7234\n",
            "19223.3340\n",
            "19482.6953\n",
            "19093.0414\n",
            "19353.8302\n",
            "18966.7536\n",
            "19586.5967\n",
            "19824.5567\n",
            "20519.5045\n",
            "21874.6371\n",
            "22426.4327\n",
            "23151.3266\n",
            "23326.6128\n",
            "23498.2918\n",
            "24190.1866\n",
            "24344.4654\n",
            "25620.9224\n",
            "26096.8313\n",
            "25574.8946\n",
            "25701.3754\n",
            "26359.9375\n",
            "26828.1617\n",
            "27481.3362\n",
            "28104.2678\n",
            "28611.3195\n",
            "29802.1878\n",
            "30378.6900\n",
            "31534.2044\n",
            "31445.9364\n",
            "31387.1649\n",
            "31637.0914\n",
            "31551.5596\n",
            "31976.2157\n",
            "32585.4817\n",
            "32444.8578\n",
            "32832.6475\n",
            "33060.1330\n",
            "33407.5232\n",
            "33747.1368\n",
            "34300.6444\n",
            "34620.0322\n",
            "34751.7591\n",
            "34056.7239\n",
            "33859.0948\n",
            "34019.5365\n",
            "33339.1458\n",
            "33698.8208\n",
            "34526.4508\n",
            "34839.6534\n",
            "34667.8530\n",
            "34978.1702\n",
            "34522.3062\n",
            "34075.5559\n",
            "34342.1661\n",
            "34492.5764\n",
            "33802.7249\n",
            "34628.0502\n",
            "34772.7349\n",
            "35066.5482\n",
            "34890.1657\n",
            "35124.9705\n",
            "35789.5380\n",
            "35937.8961\n",
            "35590.0461\n",
            "35577.8437\n",
            "35733.0069\n",
            "35846.5638\n",
            "35817.5889\n",
            "36164.8392\n",
            "36121.0387\n",
            "35785.1122\n",
            "35229.0110\n",
            "35217.7324\n",
            "35331.9999\n",
            "35504.2995\n",
            "35612.4258\n",
            "35757.8024\n",
            "36317.4865\n",
            "35977.3758\n",
            "35417.1792\n",
            "35983.5097\n",
            "35621.0857\n",
            "35787.0839\n",
            "35748.0956\n",
            "35419.3171\n",
            "35096.3181\n",
            "34553.7295\n",
            "34555.0832\n",
            "34681.8195\n",
            "34823.8003\n",
            "34979.9428\n",
            "34439.6808\n",
            "34377.7676\n",
            "33999.6717\n",
            "33583.3513\n",
            "33854.0531\n",
            "33176.9720\n",
            "32768.5026\n",
            "32872.8181\n",
            "32772.3537\n",
            "32689.5044\n",
            "33125.9574\n",
            "32750.6561\n",
            "32821.6907\n",
            "32736.0273\n",
            "32787.6542\n",
            "32195.7601\n",
            "31615.6973\n",
            "31047.2308\n",
            "30996.6449\n",
            "31129.5596\n",
            "31232.1684\n",
            "31360.3397\n",
            "31286.5582\n",
            "31367.0551\n",
            "30803.5536\n",
            "30912.6493\n",
            "31000.3068\n",
            "30950.5940\n",
            "30618.1602\n",
            "30730.9597\n",
            "30116.3405\n",
            "30067.4235\n",
            "29466.0750\n",
            "28876.7535\n",
            "28299.2185\n",
            "28227.4396\n",
            "28142.0166\n",
            "27579.1763\n",
            "27504.3580\n",
            "27775.1874\n",
            "27219.6836\n",
            "26895.1096\n",
            "26546.6790\n",
            "26651.7030\n",
            "26747.2137\n",
            "26679.4048\n",
            "26760.9043\n",
            "26692.6582\n",
            "26794.0608\n",
            "26742.8527\n",
            "26843.1950\n",
            "26773.2025\n",
            "26852.6087\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3558194086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# logits: [1, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mactive_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_expert_latent_activated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1928226868.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, state_flat)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m#             acc = torch.einsum('b, b S D-> b S D', w[mask, i], self.state_experts[e](latent[mask]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m#             mixed = mixed.index_put((mask,), acc, accumulate=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mmixed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_experts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3004600113.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, w, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mW_bank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [E, D, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mb_bank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [E, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/traceback.py\u001b[0m in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mwhose\u001b[0m \u001b[0msource\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/traceback.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mframe_summary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mformatted_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_frame_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mformatted_frame\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/traceback.py\u001b[0m in \u001b[0;36mformat_frame_summary\u001b[0;34m(self, frame_summary)\u001b[0m\n\u001b[1;32m    468\u001b[0m             frame_summary.filename, frame_summary.lineno, frame_summary.name))\n\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mstripped_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_summary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstripped_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.autograd.set_detect_anomaly(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idQILM7FHys0",
        "outputId": "39226bfa-b6d5-476c-f571-cb45528f3df6"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7e3043f48dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc\n",
        "\n",
        "def list_tensors_on_gpu():\n",
        "    tensors = []\n",
        "    for obj in gc.get_objects():\n",
        "        try:\n",
        "            if torch.is_tensor(obj) and obj.is_cuda:\n",
        "                tensors.append(obj)\n",
        "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data) and obj.data.is_cuda:\n",
        "                tensors.append(obj.data)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # Sort tensors by memory size (largest first)\n",
        "    tensors = sorted(tensors, key=lambda t: t.numel() * t.element_size(), reverse=True)\n",
        "\n",
        "    total = 0\n",
        "    print(f\"{'Idx':>3} {'Shape':>20} {'Dtype':>10} {'Size (MB)':>12} {'Device':>10}\")\n",
        "    print(\"-\" * 60)\n",
        "    for i, t in enumerate(tensors):\n",
        "        size_mb = t.numel() * t.element_size() / (1024 ** 2)\n",
        "        total += size_mb\n",
        "        print(f\"{i:3d} {str(tuple(t.shape)):>20} {str(t.dtype):>10} {size_mb:12.2f} {str(t.device):>10}\")\n",
        "\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"Total GPU tensor memory: {total:.2f} MB\")\n",
        "\n",
        "list_tensors_on_gpu()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qukQ5jIa7EB_",
        "outputId": "36f94fc9-bda4-4287-ba5b-c9a8aa6a879c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1125: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  return isinstance(obj, torch.Tensor)\n",
            "/tmp/ipython-input-2228758457.py:9: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
            "  elif hasattr(obj, \"data\") and torch.is_tensor(obj.data) and obj.data.is_cuda:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Idx                Shape      Dtype    Size (MB)     Device\n",
            "------------------------------------------------------------\n",
            "  0     (1, 4096, 12288) torch.float32       192.00     cuda:0\n",
            "  1      (1, 4096, 4096) torch.float32        64.00     cuda:0\n",
            "  2      (1, 4096, 4096) torch.float32        64.00     cuda:0\n",
            "  3      (1, 4096, 4096) torch.float32        64.00     cuda:0\n",
            "  4      (1, 4096, 4096) torch.float32        64.00     cuda:0\n",
            "  5      (1, 4096, 4096) torch.float32        64.00     cuda:0\n",
            "  6      (1, 4096, 4096) torch.float32        64.00     cuda:0\n",
            "  7      (1, 4096, 4096) torch.float32        64.00     cuda:0\n",
            "  8      (1, 8192, 1024) torch.float32        32.00     cuda:0\n",
            "  9       (1, 8192, 768) torch.float32        24.00     cuda:0\n",
            " 10       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 11       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 12       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 13       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 14       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 15       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 16       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 17       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 18       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 19       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 20       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 21       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 22       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 23       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 24       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 25       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 26       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 27       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 28       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 29       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 30       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 31       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 32       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 33       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 34       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 35       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 36       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 37       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 38       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 39       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 40       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 41       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 42       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 43       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 44       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 45       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 46       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 47       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 48       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 49       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 50       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 51       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 52       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 53       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 54       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 55       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 56       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 57       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 58       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 59       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 60       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 61       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 62       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 63       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 64       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 65       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 66       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 67       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 68       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 69       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 70       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 71       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 72       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 73       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 74       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 75       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 76       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 77       (1, 8192, 256) torch.float32         8.00     cuda:0\n",
            " 78      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 79      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 80      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 81      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 82      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 83      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 84      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 85      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 86      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 87      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 88      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 89      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 90      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 91      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 92      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 93      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 94      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 95      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 96      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 97      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 98      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            " 99      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "100      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "101      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "102      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "103      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "104      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "105      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "106      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "107      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "108      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "109      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "110      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "111      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "112      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "113      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "114      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "115      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "116      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "117      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "118      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "119      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "120      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "121      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "122      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "123      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "124      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "125      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "126      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "127      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "128      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "129      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "130      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "131      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "132      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "133      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "134      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "135      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "136      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "137      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "138      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "139      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "140      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "141      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "142      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "143      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "144      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "145      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "146      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "147      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "148      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "149      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "150      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "151      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "152      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "153      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "154      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "155      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "156      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "157      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "158      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "159      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "160      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "161      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "162      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "163      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "164      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "165      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "166      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "167      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "168      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "169      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "170      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "171      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "172      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "173      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "174      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "175      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "176      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "177      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "178      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "179      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "180      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "181      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "182      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "183      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "184      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "185      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "186      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "187      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "188      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "189      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "190      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "191      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "192      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "193      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "194      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "195      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "196      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "197      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "198      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "199      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "200      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "201      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "202      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "203      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "204      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "205      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "206      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "207      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "208      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "209      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "210      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "211      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "212      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "213      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "214      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "215      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "216      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "217      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "218      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "219      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "220      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "221      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "222      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "223      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "224      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "225      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "226      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "227      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "228      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "229      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "230      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "231      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "232      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "233      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "234      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "235      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "236      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "237      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "238      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "239      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "240      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "241      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "242      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "243      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "244      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "245      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "246      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "247      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "248      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "249      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "250      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "251      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "252      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "253      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "254      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "255      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "256      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "257      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "258      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "259      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "260      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "261      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "262      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "263      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "264      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "265      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "266      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "267      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "268      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "269      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "270      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "271      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "272      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "273      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "274      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "275      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "276      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "277      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "278      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "279      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "280      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "281      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "282      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "283      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "284      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "285      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "286      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "287      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "288      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "289      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "290      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "291      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "292      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "293      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "294      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "295      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "296      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "297      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "298      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "299      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "300      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "301      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "302      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "303      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "304      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "305      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "306      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "307      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "308      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "309      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "310      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "311      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "312      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "313      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "314      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "315      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "316      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "317      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "318      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "319      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "320      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "321      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "322      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "323      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "324      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "325      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "326      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "327      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "328      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "329      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "330      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "331      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "332      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "333      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "334      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "335      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "336      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "337      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "338      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "339      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "340      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "341      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "342      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "343      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "344      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "345      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "346      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "347      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "348      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "349      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "350      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "351      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "352      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "353      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "354      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "355      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "356      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "357      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "358      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "359      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "360      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "361      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "362      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "363      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "364      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "365      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "366      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "367      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "368      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "369      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "370      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "371      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "372      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "373      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "374      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "375      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "376      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "377      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "378      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "379      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "380      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "381      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "382      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "383      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "384      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "385      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "386      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "387      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "388      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "389      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "390      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "391      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "392      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "393      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "394      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "395      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "396      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "397      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "398      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "399      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "400      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "401      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "402      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "403      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "404      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "405      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "406      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "407      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "408      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "409      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "410      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "411      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "412      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "413      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "414      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "415      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "416      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "417      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "418      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "419      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "420      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "421      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "422      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "423      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "424      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "425      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "426      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "427      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "428      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "429      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "430      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "431      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "432      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "433      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "434      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "435      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "436      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "437      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "438      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "439      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "440      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "441      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "442      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "443      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "444      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "445      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "446      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "447      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "448      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "449      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "450      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "451      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "452      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "453      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "454      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "455      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "456      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "457      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "458      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "459      (1, 1024, 1024) torch.float32         4.00     cuda:0\n",
            "460       (1, 4096, 192) torch.float32         3.00     cuda:0\n",
            "461       (1, 1024, 768) torch.float32         3.00     cuda:0\n",
            "462       (1, 1024, 768) torch.float32         3.00     cuda:0\n",
            "463       (1, 1024, 768) torch.float32         3.00     cuda:0\n",
            "464       (1, 1024, 768) torch.float32         3.00     cuda:0\n",
            "465       (1, 1024, 768) torch.float32         3.00     cuda:0\n",
            "466        (1, 8192, 48) torch.float32         1.50     cuda:0\n",
            "467        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "468        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "469        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "470        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "471        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "472        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "473        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "474        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "475        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "476        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "477        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "478        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "479        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "480        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "481        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "482        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "483        (1, 4096, 64) torch.float32         1.00     cuda:0\n",
            "484       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "485       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "486       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "487       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "488       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "489       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "490       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "491       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "492       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "493       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "494       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "495       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "496       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "497       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "498       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "499       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "500       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "501       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "502       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "503       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "504       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "505       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "506       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "507       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "508       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "509       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "510       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "511       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "512       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "513       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "514       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "515       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "516       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "517       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "518       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "519       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "520       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "521       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "522       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "523       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "524       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "525       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "526       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "527       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "528       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "529       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "530       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "531       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "532       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "533       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "534       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "535       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "536       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "537       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "538       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "539       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "540       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "541       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "542       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "543       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "544       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "545       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "546       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "547       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "548       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "549       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "550       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "551       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "552       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "553       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "554       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "555       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "556       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "557       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "558       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "559       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "560       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "561       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "562       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "563       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "564       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "565       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "566       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "567       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "568       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "569       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "570       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "571       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "572       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "573       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "574       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "575       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "576       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "577       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "578       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "579       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "580       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "581       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "582       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "583       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "584       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "585       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "586       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "587       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "588       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "589       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "590       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "591       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "592       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "593       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "594       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "595       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "596       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "597       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "598       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "599       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "600       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "601       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "602       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "603       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "604       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "605       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "606       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "607       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "608       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "609       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "610       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "611       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "612       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "613       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "614       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "615       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "616       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "617       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "618       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "619       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "620       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "621       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "622       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "623       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "624       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "625       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "626       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "627       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "628       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "629       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "630       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "631       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "632       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "633       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "634       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "635       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "636       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "637       (1, 1024, 256) torch.float32         1.00     cuda:0\n",
            "638        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "639        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "640        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "641        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "642        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "643        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "644        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "645        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "646        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "647        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "648        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "649        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "650        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "651        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "652        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "653        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "654        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "655        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "656        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "657        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "658        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "659        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "660        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "661        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "662        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "663        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "664        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "665        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "666        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "667        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "668        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "669        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "670        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "671        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "672        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "673        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "674        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "675        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "676        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "677        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "678        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "679        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "680        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "681        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "682        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "683        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "684        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "685        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "686        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "687        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "688        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "689        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "690        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "691        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "692        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "693        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "694        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "695        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "696        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "697        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "698        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "699        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "700        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "701        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "702        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "703        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "704        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "705        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "706        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "707        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "708        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "709        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "710        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "711        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "712        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "713        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "714        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "715        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "716        (1, 8192, 16) torch.float32         0.50     cuda:0\n",
            "717        (1, 1024, 48) torch.float32         0.19     cuda:0\n",
            "718        (1, 1024, 48) torch.float32         0.19     cuda:0\n",
            "719        (1, 1024, 48) torch.float32         0.19     cuda:0\n",
            "720        (1, 1024, 48) torch.float32         0.19     cuda:0\n",
            "721        (1, 1024, 48) torch.float32         0.19     cuda:0\n",
            "722        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "723        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "724        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "725        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "726        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "727        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "728        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "729        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "730        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "731        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "732        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "733        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "734        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "735        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "736        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "737        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "738        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "739        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "740        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "741        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "742        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "743        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "744        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "745        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "746        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "747        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "748        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "749        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "750        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "751        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "752        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "753        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "754        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "755        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "756        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "757        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "758        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "759        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "760        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "761        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "762        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "763        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "764        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "765        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "766        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "767        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "768        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "769        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "770        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "771        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "772        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "773        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "774        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "775        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "776        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "777        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "778        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "779        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "780        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "781        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "782        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "783        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "784        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "785        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "786        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "787        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "788        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "789        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "790        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "791        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "792        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "793        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "794        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "795        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "796        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "797        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "798        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "799        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "800        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "801        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "802        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "803        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "804        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "805        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "806        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "807        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "808        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "809        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "810        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "811        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "812        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "813        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "814        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "815        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "816        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "817        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "818        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "819        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "820        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "821        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "822        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "823        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "824        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "825        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "826        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "827        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "828        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "829        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "830        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "831        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "832        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "833        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "834        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "835        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "836        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "837        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "838        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "839        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "840        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "841        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "842        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "843        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "844        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "845        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "846        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "847        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "848        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "849        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "850        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "851        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "852        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "853        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "854        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "855        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "856        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "857        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "858        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "859        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "860        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "861        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "862        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "863        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "864        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "865        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "866        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "867        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "868        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "869        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "870        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "871        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "872        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "873        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "874        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "875        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "876        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "877        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "878        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "879        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "880        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "881        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "882        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "883        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "884        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "885        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "886        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "887        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "888        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "889        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "890        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "891        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "892        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "893        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "894        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "895        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "896        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "897        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "898        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "899        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "900        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "901        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "902        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "903        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "904        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "905        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "906        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "907        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "908        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "909        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "910        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "911        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "912        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "913        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "914        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "915        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "916        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "917        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "918        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "919        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "920        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "921        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "922        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "923        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "924        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "925        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "926        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "927        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "928        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "929        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "930        (1, 1024, 16) torch.float32         0.06     cuda:0\n",
            "931            (192, 64) torch.float32         0.05     cuda:0\n",
            "932            (192, 64) torch.float32         0.05     cuda:0\n",
            "933              (5000,) torch.int64         0.04     cuda:0\n",
            "934              (5000,) torch.int64         0.04     cuda:0\n",
            "935       (2, 72, 1, 64) torch.float32         0.04     cuda:0\n",
            "936       (1, 64, 2, 64) torch.float32         0.03     cuda:0\n",
            "937          (1, 72, 64) torch.float32         0.02     cuda:0\n",
            "938          (1, 72, 64) torch.float32         0.02     cuda:0\n",
            "939       (1, 4, 72, 16) torch.float32         0.02     cuda:0\n",
            "940       (1, 4, 72, 16) torch.float32         0.02     cuda:0\n",
            "941            (1, 4096) torch.float32         0.02     cuda:0\n",
            "942             (64, 64) torch.float32         0.02     cuda:0\n",
            "943             (64, 64) torch.float32         0.02     cuda:0\n",
            "944             (64, 64) torch.float32         0.02     cuda:0\n",
            "945             (64, 64) torch.float32         0.02     cuda:0\n",
            "946             (64, 64) torch.float32         0.02     cuda:0\n",
            "947             (64, 64) torch.float32         0.02     cuda:0\n",
            "948             (64, 64) torch.float32         0.02     cuda:0\n",
            "949             (64, 64) torch.float32         0.02     cuda:0\n",
            "950             (64, 64) torch.float32         0.02     cuda:0\n",
            "951             (64, 64) torch.float32         0.02     cuda:0\n",
            "952             (64, 64) torch.float32         0.02     cuda:0\n",
            "953             (64, 64) torch.float32         0.02     cuda:0\n",
            "954             (64, 64) torch.float32         0.02     cuda:0\n",
            "955             (64, 64) torch.float32         0.02     cuda:0\n",
            "956             (64, 64) torch.float32         0.02     cuda:0\n",
            "957             (64, 64) torch.float32         0.02     cuda:0\n",
            "958             (64, 64) torch.float32         0.02     cuda:0\n",
            "959             (64, 64) torch.float32         0.02     cuda:0\n",
            "960             (64, 64) torch.float32         0.02     cuda:0\n",
            "961             (64, 64) torch.float32         0.02     cuda:0\n",
            "962             (64, 64) torch.float32         0.02     cuda:0\n",
            "963             (64, 64) torch.float32         0.02     cuda:0\n",
            "964             (64, 64) torch.float32         0.02     cuda:0\n",
            "965             (64, 64) torch.float32         0.02     cuda:0\n",
            "966             (64, 64) torch.float32         0.02     cuda:0\n",
            "967             (64, 64) torch.float32         0.02     cuda:0\n",
            "968             (64, 64) torch.float32         0.02     cuda:0\n",
            "969             (64, 64) torch.float32         0.02     cuda:0\n",
            "970             (64, 64) torch.float32         0.02     cuda:0\n",
            "971             (64, 64) torch.float32         0.02     cuda:0\n",
            "972             (64, 64) torch.float32         0.02     cuda:0\n",
            "973             (64, 64) torch.float32         0.02     cuda:0\n",
            "974             (64, 64) torch.float32         0.02     cuda:0\n",
            "975             (64, 64) torch.float32         0.02     cuda:0\n",
            "976             (64, 64) torch.float32         0.02     cuda:0\n",
            "977             (64, 64) torch.float32         0.02     cuda:0\n",
            "978             (64, 64) torch.float32         0.02     cuda:0\n",
            "979             (64, 64) torch.float32         0.02     cuda:0\n",
            "980             (64, 64) torch.float32         0.02     cuda:0\n",
            "981             (64, 64) torch.float32         0.02     cuda:0\n",
            "982             (64, 64) torch.float32         0.02     cuda:0\n",
            "983             (64, 64) torch.float32         0.02     cuda:0\n",
            "984             (64, 64) torch.float32         0.02     cuda:0\n",
            "985             (64, 64) torch.float32         0.02     cuda:0\n",
            "986             (64, 64) torch.float32         0.02     cuda:0\n",
            "987             (64, 64) torch.float32         0.02     cuda:0\n",
            "988             (64, 64) torch.float32         0.02     cuda:0\n",
            "989             (64, 64) torch.float32         0.02     cuda:0\n",
            "990             (64, 64) torch.float32         0.02     cuda:0\n",
            "991             (64, 64) torch.float32         0.02     cuda:0\n",
            "992             (64, 64) torch.float32         0.02     cuda:0\n",
            "993             (64, 64) torch.float32         0.02     cuda:0\n",
            "994             (64, 64) torch.float32         0.02     cuda:0\n",
            "995             (64, 64) torch.float32         0.02     cuda:0\n",
            "996             (64, 64) torch.float32         0.02     cuda:0\n",
            "997             (64, 64) torch.float32         0.02     cuda:0\n",
            "998             (64, 64) torch.float32         0.02     cuda:0\n",
            "999             (64, 64) torch.float32         0.02     cuda:0\n",
            "1000             (64, 64) torch.float32         0.02     cuda:0\n",
            "1001             (64, 64) torch.float32         0.02     cuda:0\n",
            "1002             (64, 64) torch.float32         0.02     cuda:0\n",
            "1003             (64, 64) torch.float32         0.02     cuda:0\n",
            "1004          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1005          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1006       (1, 4, 64, 16) torch.float32         0.02     cuda:0\n",
            "1007       (1, 4, 64, 16) torch.float32         0.02     cuda:0\n",
            "1008             (64, 64) torch.float32         0.02     cuda:0\n",
            "1009          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1010          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1011          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1012          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1013          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1014          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1015          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1016          (1, 64, 64) torch.float32         0.02     cuda:0\n",
            "1017            (1, 4096) torch.float32         0.02     cuda:0\n",
            "1018             (64, 64) torch.float32         0.02     cuda:0\n",
            "1019             (64, 64) torch.float32         0.02     cuda:0\n",
            "1020             (64, 64) torch.float32         0.02     cuda:0\n",
            "1021             (64, 64) torch.float32         0.02     cuda:0\n",
            "1022             (64, 64) torch.float32         0.02     cuda:0\n",
            "1023             (64, 64) torch.float32         0.02     cuda:0\n",
            "1024             (64, 64) torch.float32         0.02     cuda:0\n",
            "1025             (64, 64) torch.float32         0.02     cuda:0\n",
            "1026             (64, 64) torch.float32         0.02     cuda:0\n",
            "1027             (64, 64) torch.float32         0.02     cuda:0\n",
            "1028             (64, 64) torch.float32         0.02     cuda:0\n",
            "1029             (64, 64) torch.float32         0.02     cuda:0\n",
            "1030             (64, 64) torch.float32         0.02     cuda:0\n",
            "1031             (64, 64) torch.float32         0.02     cuda:0\n",
            "1032             (64, 64) torch.float32         0.02     cuda:0\n",
            "1033             (64, 64) torch.float32         0.02     cuda:0\n",
            "1034             (64, 64) torch.float32         0.02     cuda:0\n",
            "1035             (64, 64) torch.float32         0.02     cuda:0\n",
            "1036             (64, 64) torch.float32         0.02     cuda:0\n",
            "1037             (64, 64) torch.float32         0.02     cuda:0\n",
            "1038             (64, 64) torch.float32         0.02     cuda:0\n",
            "1039             (64, 64) torch.float32         0.02     cuda:0\n",
            "1040             (64, 64) torch.float32         0.02     cuda:0\n",
            "1041             (64, 64) torch.float32         0.02     cuda:0\n",
            "1042             (64, 64) torch.float32         0.02     cuda:0\n",
            "1043             (64, 64) torch.float32         0.02     cuda:0\n",
            "1044             (64, 64) torch.float32         0.02     cuda:0\n",
            "1045             (64, 64) torch.float32         0.02     cuda:0\n",
            "1046             (64, 64) torch.float32         0.02     cuda:0\n",
            "1047             (64, 64) torch.float32         0.02     cuda:0\n",
            "1048             (64, 64) torch.float32         0.02     cuda:0\n",
            "1049             (64, 64) torch.float32         0.02     cuda:0\n",
            "1050             (64, 64) torch.float32         0.02     cuda:0\n",
            "1051             (64, 64) torch.float32         0.02     cuda:0\n",
            "1052             (64, 64) torch.float32         0.02     cuda:0\n",
            "1053             (64, 64) torch.float32         0.02     cuda:0\n",
            "1054             (64, 64) torch.float32         0.02     cuda:0\n",
            "1055             (64, 64) torch.float32         0.02     cuda:0\n",
            "1056             (64, 64) torch.float32         0.02     cuda:0\n",
            "1057             (64, 64) torch.float32         0.02     cuda:0\n",
            "1058             (64, 64) torch.float32         0.02     cuda:0\n",
            "1059             (64, 64) torch.float32         0.02     cuda:0\n",
            "1060             (64, 64) torch.float32         0.02     cuda:0\n",
            "1061             (64, 64) torch.float32         0.02     cuda:0\n",
            "1062             (64, 64) torch.float32         0.02     cuda:0\n",
            "1063             (64, 64) torch.float32         0.02     cuda:0\n",
            "1064             (64, 64) torch.float32         0.02     cuda:0\n",
            "1065             (64, 64) torch.float32         0.02     cuda:0\n",
            "1066             (64, 64) torch.float32         0.02     cuda:0\n",
            "1067             (64, 64) torch.float32         0.02     cuda:0\n",
            "1068             (64, 64) torch.float32         0.02     cuda:0\n",
            "1069             (64, 64) torch.float32         0.02     cuda:0\n",
            "1070             (64, 64) torch.float32         0.02     cuda:0\n",
            "1071             (64, 64) torch.float32         0.02     cuda:0\n",
            "1072             (64, 64) torch.float32         0.02     cuda:0\n",
            "1073             (64, 64) torch.float32         0.02     cuda:0\n",
            "1074             (64, 64) torch.float32         0.02     cuda:0\n",
            "1075             (64, 64) torch.float32         0.02     cuda:0\n",
            "1076             (64, 64) torch.float32         0.02     cuda:0\n",
            "1077             (64, 64) torch.float32         0.02     cuda:0\n",
            "1078             (64, 64) torch.float32         0.02     cuda:0\n",
            "1079             (64, 64) torch.float32         0.02     cuda:0\n",
            "1080             (64, 64) torch.float32         0.02     cuda:0\n",
            "1081             (64, 64) torch.float32         0.02     cuda:0\n",
            "1082             (64, 64) torch.float32         0.02     cuda:0\n",
            "1083             (64, 64) torch.float32         0.02     cuda:0\n",
            "1084             (64, 64) torch.float32         0.02     cuda:0\n",
            "1085             (64, 64) torch.float32         0.02     cuda:0\n",
            "1086             (64, 64) torch.float32         0.02     cuda:0\n",
            "1087            (1, 4096) torch.float32         0.02     cuda:0\n",
            "1088             (64, 64) torch.float32         0.02     cuda:0\n",
            "1089             (64, 64) torch.float32         0.02     cuda:0\n",
            "1090             (64, 64) torch.float32         0.02     cuda:0\n",
            "1091             (64, 64) torch.float32         0.02     cuda:0\n",
            "1092             (64, 64) torch.float32         0.02     cuda:0\n",
            "1093             (64, 64) torch.float32         0.02     cuda:0\n",
            "1094             (64, 64) torch.float32         0.02     cuda:0\n",
            "1095       (2, 65, 1, 16) torch.float32         0.01     cuda:0\n",
            "1096       (2, 65, 1, 16) torch.float32         0.01     cuda:0\n",
            "1097       (1, 64, 2, 16) torch.float32         0.01     cuda:0\n",
            "1098              (1024,) torch.int64         0.01     cuda:0\n",
            "1099              (1024,) torch.int64         0.01     cuda:0\n",
            "1100              (1024,) torch.int64         0.01     cuda:0\n",
            "1101              (1024,) torch.int64         0.01     cuda:0\n",
            "1102              (1024,) torch.int64         0.01     cuda:0\n",
            "1103              (1024,) torch.int64         0.01     cuda:0\n",
            "1104              (1024,) torch.int64         0.01     cuda:0\n",
            "1105              (1024,) torch.int64         0.01     cuda:0\n",
            "1106              (1024,) torch.int64         0.01     cuda:0\n",
            "1107              (1024,) torch.int64         0.01     cuda:0\n",
            "1108              (1024,) torch.int64         0.01     cuda:0\n",
            "1109              (1024,) torch.int64         0.01     cuda:0\n",
            "1110              (1024,) torch.int64         0.01     cuda:0\n",
            "1111              (1024,) torch.int64         0.01     cuda:0\n",
            "1112              (1024,) torch.int64         0.01     cuda:0\n",
            "1113              (1024,) torch.int64         0.01     cuda:0\n",
            "1114              (1024,) torch.int64         0.01     cuda:0\n",
            "1115              (1024,) torch.int64         0.01     cuda:0\n",
            "1116              (1024,) torch.int64         0.01     cuda:0\n",
            "1117              (1024,) torch.int64         0.01     cuda:0\n",
            "1118              (1024,) torch.int64         0.01     cuda:0\n",
            "1119              (1024,) torch.int64         0.01     cuda:0\n",
            "1120              (1024,) torch.int64         0.01     cuda:0\n",
            "1121              (1024,) torch.int64         0.01     cuda:0\n",
            "1122              (1024,) torch.int64         0.01     cuda:0\n",
            "1123              (1024,) torch.int64         0.01     cuda:0\n",
            "1124              (1024,) torch.int64         0.01     cuda:0\n",
            "1125              (1024,) torch.int64         0.01     cuda:0\n",
            "1126              (1024,) torch.int64         0.01     cuda:0\n",
            "1127              (1024,) torch.int64         0.01     cuda:0\n",
            "1128              (1024,) torch.int64         0.01     cuda:0\n",
            "1129              (1024,) torch.int64         0.01     cuda:0\n",
            "1130              (1024,) torch.int64         0.01     cuda:0\n",
            "1131              (1024,) torch.int64         0.01     cuda:0\n",
            "1132              (1024,) torch.int64         0.01     cuda:0\n",
            "1133              (1024,) torch.int64         0.01     cuda:0\n",
            "1134              (1024,) torch.int64         0.01     cuda:0\n",
            "1135              (1024,) torch.int64         0.01     cuda:0\n",
            "1136              (1024,) torch.int64         0.01     cuda:0\n",
            "1137              (1024,) torch.int64         0.01     cuda:0\n",
            "1138              (1024,) torch.int64         0.01     cuda:0\n",
            "1139              (1024,) torch.int64         0.01     cuda:0\n",
            "1140              (1024,) torch.int64         0.01     cuda:0\n",
            "1141              (1024,) torch.int64         0.01     cuda:0\n",
            "1142              (1024,) torch.int64         0.01     cuda:0\n",
            "1143              (1024,) torch.int64         0.01     cuda:0\n",
            "1144              (1024,) torch.int64         0.01     cuda:0\n",
            "1145              (1024,) torch.int64         0.01     cuda:0\n",
            "1146              (1024,) torch.int64         0.01     cuda:0\n",
            "1147              (1024,) torch.int64         0.01     cuda:0\n",
            "1148              (1024,) torch.int64         0.01     cuda:0\n",
            "1149              (1024,) torch.int64         0.01     cuda:0\n",
            "1150              (1024,) torch.int64         0.01     cuda:0\n",
            "1151              (1024,) torch.int64         0.01     cuda:0\n",
            "1152              (1024,) torch.int64         0.01     cuda:0\n",
            "1153              (1024,) torch.int64         0.01     cuda:0\n",
            "1154              (1024,) torch.int64         0.01     cuda:0\n",
            "1155              (1024,) torch.int64         0.01     cuda:0\n",
            "1156              (1024,) torch.int64         0.01     cuda:0\n",
            "1157              (1024,) torch.int64         0.01     cuda:0\n",
            "1158              (1024,) torch.int64         0.01     cuda:0\n",
            "1159              (1024,) torch.int64         0.01     cuda:0\n",
            "1160              (1024,) torch.int64         0.01     cuda:0\n",
            "1161              (1024,) torch.int64         0.01     cuda:0\n",
            "1162              (1024,) torch.int64         0.01     cuda:0\n",
            "1163              (1024,) torch.int64         0.01     cuda:0\n",
            "1164              (1024,) torch.int64         0.01     cuda:0\n",
            "1165              (1024,) torch.int64         0.01     cuda:0\n",
            "1166              (1024,) torch.int64         0.01     cuda:0\n",
            "1167              (1024,) torch.int64         0.01     cuda:0\n",
            "1168              (1024,) torch.int64         0.01     cuda:0\n",
            "1169              (1024,) torch.int64         0.01     cuda:0\n",
            "1170              (1024,) torch.int64         0.01     cuda:0\n",
            "1171              (1024,) torch.int64         0.01     cuda:0\n",
            "1172              (1024,) torch.int64         0.01     cuda:0\n",
            "1173              (1024,) torch.int64         0.01     cuda:0\n",
            "1174              (1024,) torch.int64         0.01     cuda:0\n",
            "1175              (1024,) torch.int64         0.01     cuda:0\n",
            "1176              (1024,) torch.int64         0.01     cuda:0\n",
            "1177              (1024,) torch.int64         0.01     cuda:0\n",
            "1178              (1024,) torch.int64         0.01     cuda:0\n",
            "1179              (1024,) torch.int64         0.01     cuda:0\n",
            "1180              (1024,) torch.int64         0.01     cuda:0\n",
            "1181              (1024,) torch.int64         0.01     cuda:0\n",
            "1182              (1024,) torch.int64         0.01     cuda:0\n",
            "1183              (1024,) torch.int64         0.01     cuda:0\n",
            "1184              (1024,) torch.int64         0.01     cuda:0\n",
            "1185              (1024,) torch.int64         0.01     cuda:0\n",
            "1186              (1024,) torch.int64         0.01     cuda:0\n",
            "1187              (1024,) torch.int64         0.01     cuda:0\n",
            "1188              (1024,) torch.int64         0.01     cuda:0\n",
            "1189              (1024,) torch.int64         0.01     cuda:0\n",
            "1190              (1024,) torch.int64         0.01     cuda:0\n",
            "1191              (1024,) torch.int64         0.01     cuda:0\n",
            "1192              (1024,) torch.int64         0.01     cuda:0\n",
            "1193              (1024,) torch.int64         0.01     cuda:0\n",
            "1194              (1024,) torch.int64         0.01     cuda:0\n",
            "1195              (1024,) torch.int64         0.01     cuda:0\n",
            "1196              (1024,) torch.int64         0.01     cuda:0\n",
            "1197              (1024,) torch.int64         0.01     cuda:0\n",
            "1198              (1024,) torch.int64         0.01     cuda:0\n",
            "1199              (1024,) torch.int64         0.01     cuda:0\n",
            "1200              (1024,) torch.int64         0.01     cuda:0\n",
            "1201              (1024,) torch.int64         0.01     cuda:0\n",
            "1202              (1024,) torch.int64         0.01     cuda:0\n",
            "1203              (1024,) torch.int64         0.01     cuda:0\n",
            "1204              (1024,) torch.int64         0.01     cuda:0\n",
            "1205              (1024,) torch.int64         0.01     cuda:0\n",
            "1206              (1024,) torch.int64         0.01     cuda:0\n",
            "1207              (1024,) torch.int64         0.01     cuda:0\n",
            "1208              (1024,) torch.int64         0.01     cuda:0\n",
            "1209              (1024,) torch.int64         0.01     cuda:0\n",
            "1210              (1024,) torch.int64         0.01     cuda:0\n",
            "1211              (1024,) torch.int64         0.01     cuda:0\n",
            "1212              (1024,) torch.int64         0.01     cuda:0\n",
            "1213              (1024,) torch.int64         0.01     cuda:0\n",
            "1214              (1024,) torch.int64         0.01     cuda:0\n",
            "1215              (1024,) torch.int64         0.01     cuda:0\n",
            "1216       (1, 64, 2, 16) torch.float32         0.01     cuda:0\n",
            "1217              (1024,) torch.int64         0.01     cuda:0\n",
            "1218              (1024,) torch.int64         0.01     cuda:0\n",
            "1219              (1024,) torch.int64         0.01     cuda:0\n",
            "1220          (1, 65, 16) torch.float32         0.00     cuda:0\n",
            "1221          (1, 65, 16) torch.float32         0.00     cuda:0\n",
            "1222        (1, 4, 65, 4) torch.float32         0.00     cuda:0\n",
            "1223        (1, 4, 65, 4) torch.float32         0.00     cuda:0\n",
            "1224          (1, 65, 16) torch.float32         0.00     cuda:0\n",
            "1225          (1, 65, 16) torch.float32         0.00     cuda:0\n",
            "1226        (1, 4, 65, 4) torch.float32         0.00     cuda:0\n",
            "1227        (1, 4, 65, 4) torch.float32         0.00     cuda:0\n",
            "1228          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1229          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1230        (1, 4, 64, 4) torch.float32         0.00     cuda:0\n",
            "1231        (1, 4, 64, 4) torch.float32         0.00     cuda:0\n",
            "1232             (64, 16) torch.float32         0.00     cuda:0\n",
            "1233          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1234          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1235          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1236          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1237          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1238          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1239          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1240          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1241            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1242            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1243             (64, 16) torch.float32         0.00     cuda:0\n",
            "1244            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1245            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1246            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1247            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1248            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1249            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1250            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1251             (64, 16) torch.float32         0.00     cuda:0\n",
            "1252            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1253            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1254            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1255            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1256            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1257            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1258            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1259            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1260            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1261            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1262            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1263            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1264            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1265            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1266            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1267            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1268            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1269            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1270            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1271            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1272            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1273            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1274            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1275            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1276            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1277            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1278            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1279            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1280            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1281            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1282            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1283            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1284            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1285            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1286            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1287            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1288            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1289            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1290            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1291            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1292            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1293            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1294            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1295            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1296            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1297            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1298            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1299            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1300            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1301            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1302            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1303            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1304            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1305             (64, 16) torch.float32         0.00     cuda:0\n",
            "1306            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1307            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1308            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1309            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1310          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1311          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1312        (1, 4, 64, 4) torch.float32         0.00     cuda:0\n",
            "1313        (1, 4, 64, 4) torch.float32         0.00     cuda:0\n",
            "1314             (64, 16) torch.float32         0.00     cuda:0\n",
            "1315          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1316          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1317          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1318          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1319          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1320          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1321          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1322          (1, 64, 16) torch.float32         0.00     cuda:0\n",
            "1323            (1, 1024) torch.float32         0.00     cuda:0\n",
            "1324             (48, 16) torch.float32         0.00     cuda:0\n",
            "1325             (48, 16) torch.float32         0.00     cuda:0\n",
            "1326             (48, 16) torch.float32         0.00     cuda:0\n",
            "1327             (32, 16) torch.float32         0.00     cuda:0\n",
            "1328             (32, 16) torch.float32         0.00     cuda:0\n",
            "1329             (32, 16) torch.float32         0.00     cuda:0\n",
            "1330             (16, 16) torch.float32         0.00     cuda:0\n",
            "1331             (16, 16) torch.float32         0.00     cuda:0\n",
            "1332             (16, 16) torch.float32         0.00     cuda:0\n",
            "1333             (16, 16) torch.float32         0.00     cuda:0\n",
            "1334             (16, 16) torch.float32         0.00     cuda:0\n",
            "1335             (16, 16) torch.float32         0.00     cuda:0\n",
            "1336             (16, 16) torch.float32         0.00     cuda:0\n",
            "1337             (16, 16) torch.float32         0.00     cuda:0\n",
            "1338             (16, 16) torch.float32         0.00     cuda:0\n",
            "1339             (16, 16) torch.float32         0.00     cuda:0\n",
            "1340             (16, 16) torch.float32         0.00     cuda:0\n",
            "1341             (16, 16) torch.float32         0.00     cuda:0\n",
            "1342             (16, 16) torch.float32         0.00     cuda:0\n",
            "1343             (16, 16) torch.float32         0.00     cuda:0\n",
            "1344             (16, 16) torch.float32         0.00     cuda:0\n",
            "1345             (16, 16) torch.float32         0.00     cuda:0\n",
            "1346             (16, 16) torch.float32         0.00     cuda:0\n",
            "1347             (16, 16) torch.float32         0.00     cuda:0\n",
            "1348             (16, 16) torch.float32         0.00     cuda:0\n",
            "1349             (16, 16) torch.float32         0.00     cuda:0\n",
            "1350             (16, 16) torch.float32         0.00     cuda:0\n",
            "1351             (16, 16) torch.float32         0.00     cuda:0\n",
            "1352             (16, 16) torch.float32         0.00     cuda:0\n",
            "1353             (16, 16) torch.float32         0.00     cuda:0\n",
            "1354             (16, 16) torch.float32         0.00     cuda:0\n",
            "1355             (16, 16) torch.float32         0.00     cuda:0\n",
            "1356             (16, 16) torch.float32         0.00     cuda:0\n",
            "1357             (16, 16) torch.float32         0.00     cuda:0\n",
            "1358             (16, 16) torch.float32         0.00     cuda:0\n",
            "1359             (16, 16) torch.float32         0.00     cuda:0\n",
            "1360             (16, 16) torch.float32         0.00     cuda:0\n",
            "1361             (16, 16) torch.float32         0.00     cuda:0\n",
            "1362             (16, 16) torch.float32         0.00     cuda:0\n",
            "1363             (16, 16) torch.float32         0.00     cuda:0\n",
            "1364             (16, 16) torch.float32         0.00     cuda:0\n",
            "1365             (16, 16) torch.float32         0.00     cuda:0\n",
            "1366             (16, 16) torch.float32         0.00     cuda:0\n",
            "1367             (16, 16) torch.float32         0.00     cuda:0\n",
            "1368             (16, 16) torch.float32         0.00     cuda:0\n",
            "1369             (16, 16) torch.float32         0.00     cuda:0\n",
            "1370             (16, 16) torch.float32         0.00     cuda:0\n",
            "1371             (16, 16) torch.float32         0.00     cuda:0\n",
            "1372             (16, 16) torch.float32         0.00     cuda:0\n",
            "1373             (16, 16) torch.float32         0.00     cuda:0\n",
            "1374             (16, 16) torch.float32         0.00     cuda:0\n",
            "1375             (16, 16) torch.float32         0.00     cuda:0\n",
            "1376             (16, 16) torch.float32         0.00     cuda:0\n",
            "1377             (16, 16) torch.float32         0.00     cuda:0\n",
            "1378             (16, 16) torch.float32         0.00     cuda:0\n",
            "1379             (16, 16) torch.float32         0.00     cuda:0\n",
            "1380             (16, 16) torch.float32         0.00     cuda:0\n",
            "1381             (16, 16) torch.float32         0.00     cuda:0\n",
            "1382             (16, 16) torch.float32         0.00     cuda:0\n",
            "1383             (16, 16) torch.float32         0.00     cuda:0\n",
            "1384             (16, 16) torch.float32         0.00     cuda:0\n",
            "1385             (16, 16) torch.float32         0.00     cuda:0\n",
            "1386             (16, 16) torch.float32         0.00     cuda:0\n",
            "1387             (16, 16) torch.float32         0.00     cuda:0\n",
            "1388             (16, 16) torch.float32         0.00     cuda:0\n",
            "1389             (16, 16) torch.float32         0.00     cuda:0\n",
            "1390             (16, 16) torch.float32         0.00     cuda:0\n",
            "1391             (16, 16) torch.float32         0.00     cuda:0\n",
            "1392             (16, 16) torch.float32         0.00     cuda:0\n",
            "1393             (16, 16) torch.float32         0.00     cuda:0\n",
            "1394             (16, 16) torch.float32         0.00     cuda:0\n",
            "1395             (16, 16) torch.float32         0.00     cuda:0\n",
            "1396             (16, 16) torch.float32         0.00     cuda:0\n",
            "1397             (16, 16) torch.float32         0.00     cuda:0\n",
            "1398             (16, 16) torch.float32         0.00     cuda:0\n",
            "1399             (16, 16) torch.float32         0.00     cuda:0\n",
            "1400             (16, 16) torch.float32         0.00     cuda:0\n",
            "1401             (16, 16) torch.float32         0.00     cuda:0\n",
            "1402             (16, 16) torch.float32         0.00     cuda:0\n",
            "1403             (16, 16) torch.float32         0.00     cuda:0\n",
            "1404             (16, 16) torch.float32         0.00     cuda:0\n",
            "1405             (16, 16) torch.float32         0.00     cuda:0\n",
            "1406             (16, 16) torch.float32         0.00     cuda:0\n",
            "1407             (16, 16) torch.float32         0.00     cuda:0\n",
            "1408             (16, 16) torch.float32         0.00     cuda:0\n",
            "1409             (16, 16) torch.float32         0.00     cuda:0\n",
            "1410             (16, 16) torch.float32         0.00     cuda:0\n",
            "1411             (16, 16) torch.float32         0.00     cuda:0\n",
            "1412             (16, 16) torch.float32         0.00     cuda:0\n",
            "1413             (16, 16) torch.float32         0.00     cuda:0\n",
            "1414             (16, 16) torch.float32         0.00     cuda:0\n",
            "1415             (16, 16) torch.float32         0.00     cuda:0\n",
            "1416             (16, 16) torch.float32         0.00     cuda:0\n",
            "1417             (16, 16) torch.float32         0.00     cuda:0\n",
            "1418             (16, 16) torch.float32         0.00     cuda:0\n",
            "1419             (16, 16) torch.float32         0.00     cuda:0\n",
            "1420             (16, 16) torch.float32         0.00     cuda:0\n",
            "1421             (16, 16) torch.float32         0.00     cuda:0\n",
            "1422             (16, 16) torch.float32         0.00     cuda:0\n",
            "1423             (16, 16) torch.float32         0.00     cuda:0\n",
            "1424             (16, 16) torch.float32         0.00     cuda:0\n",
            "1425             (16, 16) torch.float32         0.00     cuda:0\n",
            "1426             (16, 16) torch.float32         0.00     cuda:0\n",
            "1427             (16, 16) torch.float32         0.00     cuda:0\n",
            "1428             (16, 16) torch.float32         0.00     cuda:0\n",
            "1429             (16, 16) torch.float32         0.00     cuda:0\n",
            "1430             (16, 16) torch.float32         0.00     cuda:0\n",
            "1431             (16, 16) torch.float32         0.00     cuda:0\n",
            "1432             (16, 16) torch.float32         0.00     cuda:0\n",
            "1433             (16, 16) torch.float32         0.00     cuda:0\n",
            "1434             (16, 16) torch.float32         0.00     cuda:0\n",
            "1435             (16, 16) torch.float32         0.00     cuda:0\n",
            "1436             (16, 16) torch.float32         0.00     cuda:0\n",
            "1437             (16, 16) torch.float32         0.00     cuda:0\n",
            "1438             (16, 16) torch.float32         0.00     cuda:0\n",
            "1439             (16, 16) torch.float32         0.00     cuda:0\n",
            "1440             (16, 16) torch.float32         0.00     cuda:0\n",
            "1441             (16, 16) torch.float32         0.00     cuda:0\n",
            "1442             (16, 16) torch.float32         0.00     cuda:0\n",
            "1443             (16, 16) torch.float32         0.00     cuda:0\n",
            "1444             (16, 16) torch.float32         0.00     cuda:0\n",
            "1445             (16, 16) torch.float32         0.00     cuda:0\n",
            "1446             (16, 16) torch.float32         0.00     cuda:0\n",
            "1447             (16, 16) torch.float32         0.00     cuda:0\n",
            "1448             (16, 16) torch.float32         0.00     cuda:0\n",
            "1449             (16, 16) torch.float32         0.00     cuda:0\n",
            "1450             (16, 16) torch.float32         0.00     cuda:0\n",
            "1451             (16, 16) torch.float32         0.00     cuda:0\n",
            "1452             (16, 16) torch.float32         0.00     cuda:0\n",
            "1453             (16, 16) torch.float32         0.00     cuda:0\n",
            "1454             (16, 16) torch.float32         0.00     cuda:0\n",
            "1455             (16, 16) torch.float32         0.00     cuda:0\n",
            "1456             (16, 16) torch.float32         0.00     cuda:0\n",
            "1457             (16, 16) torch.float32         0.00     cuda:0\n",
            "1458             (16, 16) torch.float32         0.00     cuda:0\n",
            "1459             (16, 16) torch.float32         0.00     cuda:0\n",
            "1460             (16, 16) torch.float32         0.00     cuda:0\n",
            "1461             (16, 16) torch.float32         0.00     cuda:0\n",
            "1462             (16, 16) torch.float32         0.00     cuda:0\n",
            "1463             (16, 16) torch.float32         0.00     cuda:0\n",
            "1464             (16, 16) torch.float32         0.00     cuda:0\n",
            "1465             (16, 16) torch.float32         0.00     cuda:0\n",
            "1466             (16, 16) torch.float32         0.00     cuda:0\n",
            "1467             (16, 16) torch.float32         0.00     cuda:0\n",
            "1468             (16, 16) torch.float32         0.00     cuda:0\n",
            "1469             (16, 16) torch.float32         0.00     cuda:0\n",
            "1470             (16, 16) torch.float32         0.00     cuda:0\n",
            "1471             (16, 16) torch.float32         0.00     cuda:0\n",
            "1472             (16, 16) torch.float32         0.00     cuda:0\n",
            "1473             (16, 16) torch.float32         0.00     cuda:0\n",
            "1474             (16, 16) torch.float32         0.00     cuda:0\n",
            "1475             (16, 16) torch.float32         0.00     cuda:0\n",
            "1476             (16, 16) torch.float32         0.00     cuda:0\n",
            "1477             (16, 16) torch.float32         0.00     cuda:0\n",
            "1478             (16, 16) torch.float32         0.00     cuda:0\n",
            "1479             (16, 16) torch.float32         0.00     cuda:0\n",
            "1480             (16, 16) torch.float32         0.00     cuda:0\n",
            "1481             (16, 16) torch.float32         0.00     cuda:0\n",
            "1482             (16, 16) torch.float32         0.00     cuda:0\n",
            "1483             (16, 16) torch.float32         0.00     cuda:0\n",
            "1484             (16, 16) torch.float32         0.00     cuda:0\n",
            "1485             (16, 16) torch.float32         0.00     cuda:0\n",
            "1486             (16, 16) torch.float32         0.00     cuda:0\n",
            "1487             (16, 16) torch.float32         0.00     cuda:0\n",
            "1488             (16, 16) torch.float32         0.00     cuda:0\n",
            "1489             (16, 16) torch.float32         0.00     cuda:0\n",
            "1490             (16, 16) torch.float32         0.00     cuda:0\n",
            "1491             (16, 16) torch.float32         0.00     cuda:0\n",
            "1492             (16, 16) torch.float32         0.00     cuda:0\n",
            "1493             (16, 16) torch.float32         0.00     cuda:0\n",
            "1494             (16, 16) torch.float32         0.00     cuda:0\n",
            "1495             (16, 16) torch.float32         0.00     cuda:0\n",
            "1496             (16, 16) torch.float32         0.00     cuda:0\n",
            "1497             (16, 16) torch.float32         0.00     cuda:0\n",
            "1498             (16, 16) torch.float32         0.00     cuda:0\n",
            "1499             (16, 16) torch.float32         0.00     cuda:0\n",
            "1500             (16, 16) torch.float32         0.00     cuda:0\n",
            "1501             (16, 16) torch.float32         0.00     cuda:0\n",
            "1502             (16, 16) torch.float32         0.00     cuda:0\n",
            "1503             (16, 16) torch.float32         0.00     cuda:0\n",
            "1504             (16, 16) torch.float32         0.00     cuda:0\n",
            "1505             (16, 16) torch.float32         0.00     cuda:0\n",
            "1506             (16, 16) torch.float32         0.00     cuda:0\n",
            "1507             (16, 16) torch.float32         0.00     cuda:0\n",
            "1508             (16, 16) torch.float32         0.00     cuda:0\n",
            "1509             (16, 16) torch.float32         0.00     cuda:0\n",
            "1510             (16, 16) torch.float32         0.00     cuda:0\n",
            "1511             (16, 16) torch.float32         0.00     cuda:0\n",
            "1512             (16, 16) torch.float32         0.00     cuda:0\n",
            "1513             (16, 16) torch.float32         0.00     cuda:0\n",
            "1514             (16, 16) torch.float32         0.00     cuda:0\n",
            "1515             (16, 16) torch.float32         0.00     cuda:0\n",
            "1516             (16, 16) torch.float32         0.00     cuda:0\n",
            "1517             (16, 16) torch.float32         0.00     cuda:0\n",
            "1518             (16, 16) torch.float32         0.00     cuda:0\n",
            "1519             (16, 16) torch.float32         0.00     cuda:0\n",
            "1520             (16, 16) torch.float32         0.00     cuda:0\n",
            "1521             (16, 16) torch.float32         0.00     cuda:0\n",
            "1522             (16, 16) torch.float32         0.00     cuda:0\n",
            "1523             (16, 16) torch.float32         0.00     cuda:0\n",
            "1524             (16, 16) torch.float32         0.00     cuda:0\n",
            "1525             (16, 16) torch.float32         0.00     cuda:0\n",
            "1526             (16, 16) torch.float32         0.00     cuda:0\n",
            "1527             (16, 16) torch.float32         0.00     cuda:0\n",
            "1528             (16, 16) torch.float32         0.00     cuda:0\n",
            "1529             (16, 16) torch.float32         0.00     cuda:0\n",
            "1530             (16, 16) torch.float32         0.00     cuda:0\n",
            "1531             (16, 16) torch.float32         0.00     cuda:0\n",
            "1532             (16, 16) torch.float32         0.00     cuda:0\n",
            "1533             (16, 16) torch.float32         0.00     cuda:0\n",
            "1534             (16, 16) torch.float32         0.00     cuda:0\n",
            "1535             (16, 16) torch.float32         0.00     cuda:0\n",
            "1536             (16, 16) torch.float32         0.00     cuda:0\n",
            "1537               (192,) torch.float32         0.00     cuda:0\n",
            "1538               (192,) torch.float32         0.00     cuda:0\n",
            "1539                (64,) torch.float32         0.00     cuda:0\n",
            "1540                (64,) torch.float32         0.00     cuda:0\n",
            "1541                (64,) torch.float32         0.00     cuda:0\n",
            "1542                (64,) torch.float32         0.00     cuda:0\n",
            "1543                (64,) torch.float32         0.00     cuda:0\n",
            "1544                (64,) torch.float32         0.00     cuda:0\n",
            "1545                (64,) torch.float32         0.00     cuda:0\n",
            "1546                (64,) torch.float32         0.00     cuda:0\n",
            "1547                (64,) torch.float32         0.00     cuda:0\n",
            "1548                (64,) torch.float32         0.00     cuda:0\n",
            "1549                (64,) torch.float32         0.00     cuda:0\n",
            "1550                (64,) torch.float32         0.00     cuda:0\n",
            "1551                (64,) torch.float32         0.00     cuda:0\n",
            "1552                (64,) torch.float32         0.00     cuda:0\n",
            "1553                (64,) torch.float32         0.00     cuda:0\n",
            "1554                (64,) torch.float32         0.00     cuda:0\n",
            "1555                (64,) torch.float32         0.00     cuda:0\n",
            "1556                (64,) torch.float32         0.00     cuda:0\n",
            "1557                (64,) torch.float32         0.00     cuda:0\n",
            "1558                (64,) torch.float32         0.00     cuda:0\n",
            "1559                (64,) torch.float32         0.00     cuda:0\n",
            "1560                (64,) torch.float32         0.00     cuda:0\n",
            "1561                (64,) torch.float32         0.00     cuda:0\n",
            "1562                (64,) torch.float32         0.00     cuda:0\n",
            "1563                (64,) torch.float32         0.00     cuda:0\n",
            "1564                (64,) torch.float32         0.00     cuda:0\n",
            "1565                (64,) torch.float32         0.00     cuda:0\n",
            "1566                (64,) torch.float32         0.00     cuda:0\n",
            "1567                (64,) torch.float32         0.00     cuda:0\n",
            "1568                (64,) torch.float32         0.00     cuda:0\n",
            "1569                (64,) torch.float32         0.00     cuda:0\n",
            "1570                (64,) torch.float32         0.00     cuda:0\n",
            "1571                (64,) torch.float32         0.00     cuda:0\n",
            "1572                (64,) torch.float32         0.00     cuda:0\n",
            "1573                (64,) torch.float32         0.00     cuda:0\n",
            "1574                (64,) torch.float32         0.00     cuda:0\n",
            "1575                (64,) torch.float32         0.00     cuda:0\n",
            "1576                (64,) torch.float32         0.00     cuda:0\n",
            "1577                (64,) torch.float32         0.00     cuda:0\n",
            "1578                (64,) torch.float32         0.00     cuda:0\n",
            "1579                (64,) torch.float32         0.00     cuda:0\n",
            "1580                (64,) torch.float32         0.00     cuda:0\n",
            "1581                (64,) torch.float32         0.00     cuda:0\n",
            "1582                (64,) torch.float32         0.00     cuda:0\n",
            "1583                (64,) torch.float32         0.00     cuda:0\n",
            "1584                (64,) torch.float32         0.00     cuda:0\n",
            "1585                (64,) torch.float32         0.00     cuda:0\n",
            "1586                (64,) torch.float32         0.00     cuda:0\n",
            "1587                (64,) torch.float32         0.00     cuda:0\n",
            "1588                (64,) torch.float32         0.00     cuda:0\n",
            "1589                (64,) torch.float32         0.00     cuda:0\n",
            "1590                (64,) torch.float32         0.00     cuda:0\n",
            "1591                (64,) torch.float32         0.00     cuda:0\n",
            "1592                (64,) torch.float32         0.00     cuda:0\n",
            "1593                (64,) torch.float32         0.00     cuda:0\n",
            "1594                (64,) torch.float32         0.00     cuda:0\n",
            "1595                (64,) torch.float32         0.00     cuda:0\n",
            "1596                (64,) torch.float32         0.00     cuda:0\n",
            "1597                (64,) torch.float32         0.00     cuda:0\n",
            "1598                (64,) torch.float32         0.00     cuda:0\n",
            "1599                (64,) torch.float32         0.00     cuda:0\n",
            "1600                (64,) torch.float32         0.00     cuda:0\n",
            "1601              (1, 64) torch.float32         0.00     cuda:0\n",
            "1602              (1, 64) torch.float32         0.00     cuda:0\n",
            "1603              (1, 64) torch.float32         0.00     cuda:0\n",
            "1604           (1, 64, 1) torch.float32         0.00     cuda:0\n",
            "1605              (1, 64) torch.float32         0.00     cuda:0\n",
            "1606                (64,) torch.float32         0.00     cuda:0\n",
            "1607                (64,) torch.float32         0.00     cuda:0\n",
            "1608                (64,) torch.float32         0.00     cuda:0\n",
            "1609                (64,) torch.float32         0.00     cuda:0\n",
            "1610                (64,) torch.float32         0.00     cuda:0\n",
            "1611                (64,) torch.float32         0.00     cuda:0\n",
            "1612                (64,) torch.float32         0.00     cuda:0\n",
            "1613                (64,) torch.float32         0.00     cuda:0\n",
            "1614                (64,) torch.float32         0.00     cuda:0\n",
            "1615                (64,) torch.float32         0.00     cuda:0\n",
            "1616                (64,) torch.float32         0.00     cuda:0\n",
            "1617                (64,) torch.float32         0.00     cuda:0\n",
            "1618                (64,) torch.float32         0.00     cuda:0\n",
            "1619                (64,) torch.float32         0.00     cuda:0\n",
            "1620                (64,) torch.float32         0.00     cuda:0\n",
            "1621                (64,) torch.float32         0.00     cuda:0\n",
            "1622                (64,) torch.float32         0.00     cuda:0\n",
            "1623                (64,) torch.float32         0.00     cuda:0\n",
            "1624                (64,) torch.float32         0.00     cuda:0\n",
            "1625                (64,) torch.float32         0.00     cuda:0\n",
            "1626                (64,) torch.float32         0.00     cuda:0\n",
            "1627                (64,) torch.float32         0.00     cuda:0\n",
            "1628                (64,) torch.float32         0.00     cuda:0\n",
            "1629                (64,) torch.float32         0.00     cuda:0\n",
            "1630                (64,) torch.float32         0.00     cuda:0\n",
            "1631                (64,) torch.float32         0.00     cuda:0\n",
            "1632                (64,) torch.float32         0.00     cuda:0\n",
            "1633                (64,) torch.float32         0.00     cuda:0\n",
            "1634                (64,) torch.float32         0.00     cuda:0\n",
            "1635                (64,) torch.float32         0.00     cuda:0\n",
            "1636                (64,) torch.float32         0.00     cuda:0\n",
            "1637                (64,) torch.float32         0.00     cuda:0\n",
            "1638                (64,) torch.float32         0.00     cuda:0\n",
            "1639                (64,) torch.float32         0.00     cuda:0\n",
            "1640                (64,) torch.float32         0.00     cuda:0\n",
            "1641                (64,) torch.float32         0.00     cuda:0\n",
            "1642                (64,) torch.float32         0.00     cuda:0\n",
            "1643                (64,) torch.float32         0.00     cuda:0\n",
            "1644                (64,) torch.float32         0.00     cuda:0\n",
            "1645                (64,) torch.float32         0.00     cuda:0\n",
            "1646                (64,) torch.float32         0.00     cuda:0\n",
            "1647                (64,) torch.float32         0.00     cuda:0\n",
            "1648                (64,) torch.float32         0.00     cuda:0\n",
            "1649                (64,) torch.float32         0.00     cuda:0\n",
            "1650                (64,) torch.float32         0.00     cuda:0\n",
            "1651                (64,) torch.float32         0.00     cuda:0\n",
            "1652                (64,) torch.float32         0.00     cuda:0\n",
            "1653                (64,) torch.float32         0.00     cuda:0\n",
            "1654                (64,) torch.float32         0.00     cuda:0\n",
            "1655                (64,) torch.float32         0.00     cuda:0\n",
            "1656                (64,) torch.float32         0.00     cuda:0\n",
            "1657                (64,) torch.float32         0.00     cuda:0\n",
            "1658                (64,) torch.float32         0.00     cuda:0\n",
            "1659                (64,) torch.float32         0.00     cuda:0\n",
            "1660                (64,) torch.float32         0.00     cuda:0\n",
            "1661                (64,) torch.float32         0.00     cuda:0\n",
            "1662                (64,) torch.float32         0.00     cuda:0\n",
            "1663                (64,) torch.float32         0.00     cuda:0\n",
            "1664                (64,) torch.float32         0.00     cuda:0\n",
            "1665                (64,) torch.float32         0.00     cuda:0\n",
            "1666                (64,) torch.float32         0.00     cuda:0\n",
            "1667                (64,) torch.float32         0.00     cuda:0\n",
            "1668                (64,) torch.float32         0.00     cuda:0\n",
            "1669                (64,) torch.float32         0.00     cuda:0\n",
            "1670                (64,) torch.float32         0.00     cuda:0\n",
            "1671                (64,) torch.float32         0.00     cuda:0\n",
            "1672                (64,) torch.float32         0.00     cuda:0\n",
            "1673                (64,) torch.float32         0.00     cuda:0\n",
            "1674                (64,) torch.float32         0.00     cuda:0\n",
            "1675                (64,) torch.float32         0.00     cuda:0\n",
            "1676                (64,) torch.float32         0.00     cuda:0\n",
            "1677                (64,) torch.float32         0.00     cuda:0\n",
            "1678                (64,) torch.float32         0.00     cuda:0\n",
            "1679                (64,) torch.float32         0.00     cuda:0\n",
            "1680                (64,) torch.float32         0.00     cuda:0\n",
            "1681                (64,) torch.float32         0.00     cuda:0\n",
            "1682                (64,) torch.float32         0.00     cuda:0\n",
            "1683                (64,) torch.float32         0.00     cuda:0\n",
            "1684              (1, 64) torch.float32         0.00     cuda:0\n",
            "1685              (1, 64) torch.float32         0.00     cuda:0\n",
            "1686           (1, 64, 1) torch.float32         0.00     cuda:0\n",
            "1687                (64,) torch.float32         0.00     cuda:0\n",
            "1688                (64,) torch.float32         0.00     cuda:0\n",
            "1689                (64,) torch.float32         0.00     cuda:0\n",
            "1690                (64,) torch.float32         0.00     cuda:0\n",
            "1691                (64,) torch.float32         0.00     cuda:0\n",
            "1692                (64,) torch.float32         0.00     cuda:0\n",
            "1693                (64,) torch.float32         0.00     cuda:0\n",
            "1694                (64,) torch.float32         0.00     cuda:0\n",
            "1695                (64,) torch.float32         0.00     cuda:0\n",
            "1696                (64,) torch.float32         0.00     cuda:0\n",
            "1697                (64,) torch.float32         0.00     cuda:0\n",
            "1698                (64,) torch.float32         0.00     cuda:0\n",
            "1699                (64,) torch.float32         0.00     cuda:0\n",
            "1700                (64,) torch.float32         0.00     cuda:0\n",
            "1701              (1, 64) torch.float32         0.00     cuda:0\n",
            "1702                (64,) torch.float32         0.00     cuda:0\n",
            "1703                (64,) torch.float32         0.00     cuda:0\n",
            "1704                (32,) torch.int64         0.00     cuda:0\n",
            "1705              (1, 64) torch.float32         0.00     cuda:0\n",
            "1706           (1, 64, 1) torch.float32         0.00     cuda:0\n",
            "1707                (48,) torch.float32         0.00     cuda:0\n",
            "1708                (48,) torch.float32         0.00     cuda:0\n",
            "1709                (48,) torch.float32         0.00     cuda:0\n",
            "1710               (3, 8) torch.float32         0.00     cuda:0\n",
            "1711              (1, 16) torch.float32         0.00     cuda:0\n",
            "1712              (1, 16) torch.float32         0.00     cuda:0\n",
            "1713                (16,) torch.float32         0.00     cuda:0\n",
            "1714                (16,) torch.float32         0.00     cuda:0\n",
            "1715                (16,) torch.float32         0.00     cuda:0\n",
            "1716                (16,) torch.float32         0.00     cuda:0\n",
            "1717                (16,) torch.float32         0.00     cuda:0\n",
            "1718                (16,) torch.float32         0.00     cuda:0\n",
            "1719                (16,) torch.float32         0.00     cuda:0\n",
            "1720                (16,) torch.float32         0.00     cuda:0\n",
            "1721                (16,) torch.float32         0.00     cuda:0\n",
            "1722                (16,) torch.float32         0.00     cuda:0\n",
            "1723                (16,) torch.float32         0.00     cuda:0\n",
            "1724                (16,) torch.float32         0.00     cuda:0\n",
            "1725                (16,) torch.float32         0.00     cuda:0\n",
            "1726                (16,) torch.float32         0.00     cuda:0\n",
            "1727                (16,) torch.float32         0.00     cuda:0\n",
            "1728                (16,) torch.float32         0.00     cuda:0\n",
            "1729                (16,) torch.float32         0.00     cuda:0\n",
            "1730                (16,) torch.float32         0.00     cuda:0\n",
            "1731                (16,) torch.float32         0.00     cuda:0\n",
            "1732                (16,) torch.float32         0.00     cuda:0\n",
            "1733                (16,) torch.float32         0.00     cuda:0\n",
            "1734                (16,) torch.float32         0.00     cuda:0\n",
            "1735                (16,) torch.float32         0.00     cuda:0\n",
            "1736                (16,) torch.float32         0.00     cuda:0\n",
            "1737                (16,) torch.float32         0.00     cuda:0\n",
            "1738                (16,) torch.float32         0.00     cuda:0\n",
            "1739                (16,) torch.float32         0.00     cuda:0\n",
            "1740                (16,) torch.float32         0.00     cuda:0\n",
            "1741                (16,) torch.float32         0.00     cuda:0\n",
            "1742                (16,) torch.float32         0.00     cuda:0\n",
            "1743                (16,) torch.float32         0.00     cuda:0\n",
            "1744                (16,) torch.float32         0.00     cuda:0\n",
            "1745                (16,) torch.float32         0.00     cuda:0\n",
            "1746                (16,) torch.float32         0.00     cuda:0\n",
            "1747                (16,) torch.float32         0.00     cuda:0\n",
            "1748                (16,) torch.float32         0.00     cuda:0\n",
            "1749                (16,) torch.float32         0.00     cuda:0\n",
            "1750                (16,) torch.float32         0.00     cuda:0\n",
            "1751                (16,) torch.float32         0.00     cuda:0\n",
            "1752                (16,) torch.float32         0.00     cuda:0\n",
            "1753                (16,) torch.float32         0.00     cuda:0\n",
            "1754                (16,) torch.float32         0.00     cuda:0\n",
            "1755                (16,) torch.float32         0.00     cuda:0\n",
            "1756                (16,) torch.float32         0.00     cuda:0\n",
            "1757                (16,) torch.float32         0.00     cuda:0\n",
            "1758                (16,) torch.float32         0.00     cuda:0\n",
            "1759                (16,) torch.float32         0.00     cuda:0\n",
            "1760                (16,) torch.float32         0.00     cuda:0\n",
            "1761                (16,) torch.float32         0.00     cuda:0\n",
            "1762                (16,) torch.float32         0.00     cuda:0\n",
            "1763                (16,) torch.float32         0.00     cuda:0\n",
            "1764                (16,) torch.float32         0.00     cuda:0\n",
            "1765                (16,) torch.float32         0.00     cuda:0\n",
            "1766                (16,) torch.float32         0.00     cuda:0\n",
            "1767                (16,) torch.float32         0.00     cuda:0\n",
            "1768                (16,) torch.float32         0.00     cuda:0\n",
            "1769                (16,) torch.float32         0.00     cuda:0\n",
            "1770                (16,) torch.float32         0.00     cuda:0\n",
            "1771                (16,) torch.float32         0.00     cuda:0\n",
            "1772                (16,) torch.float32         0.00     cuda:0\n",
            "1773                (16,) torch.float32         0.00     cuda:0\n",
            "1774                (16,) torch.float32         0.00     cuda:0\n",
            "1775                (16,) torch.float32         0.00     cuda:0\n",
            "1776                (16,) torch.float32         0.00     cuda:0\n",
            "1777                (16,) torch.float32         0.00     cuda:0\n",
            "1778                (16,) torch.float32         0.00     cuda:0\n",
            "1779                (16,) torch.float32         0.00     cuda:0\n",
            "1780                (16,) torch.float32         0.00     cuda:0\n",
            "1781                (16,) torch.float32         0.00     cuda:0\n",
            "1782                (16,) torch.float32         0.00     cuda:0\n",
            "1783                (16,) torch.float32         0.00     cuda:0\n",
            "1784                (16,) torch.float32         0.00     cuda:0\n",
            "1785                (16,) torch.float32         0.00     cuda:0\n",
            "1786                (16,) torch.float32         0.00     cuda:0\n",
            "1787                (16,) torch.float32         0.00     cuda:0\n",
            "1788                (16,) torch.float32         0.00     cuda:0\n",
            "1789                (16,) torch.float32         0.00     cuda:0\n",
            "1790              (1, 16) torch.float32         0.00     cuda:0\n",
            "1791                (16,) torch.float32         0.00     cuda:0\n",
            "1792                (16,) torch.float32         0.00     cuda:0\n",
            "1793           (1, 1, 16) torch.float32         0.00     cuda:0\n",
            "1794                (16,) torch.float32         0.00     cuda:0\n",
            "1795                (16,) torch.float32         0.00     cuda:0\n",
            "1796                (16,) torch.float32         0.00     cuda:0\n",
            "1797                (16,) torch.float32         0.00     cuda:0\n",
            "1798                (16,) torch.float32         0.00     cuda:0\n",
            "1799                (16,) torch.float32         0.00     cuda:0\n",
            "1800                (16,) torch.float32         0.00     cuda:0\n",
            "1801                (16,) torch.float32         0.00     cuda:0\n",
            "1802                (16,) torch.float32         0.00     cuda:0\n",
            "1803                (16,) torch.float32         0.00     cuda:0\n",
            "1804                (16,) torch.float32         0.00     cuda:0\n",
            "1805                (16,) torch.float32         0.00     cuda:0\n",
            "1806                (16,) torch.float32         0.00     cuda:0\n",
            "1807                (16,) torch.float32         0.00     cuda:0\n",
            "1808                (16,) torch.float32         0.00     cuda:0\n",
            "1809                (16,) torch.float32         0.00     cuda:0\n",
            "1810                (16,) torch.float32         0.00     cuda:0\n",
            "1811                (16,) torch.float32         0.00     cuda:0\n",
            "1812                (16,) torch.float32         0.00     cuda:0\n",
            "1813                (16,) torch.float32         0.00     cuda:0\n",
            "1814                (16,) torch.float32         0.00     cuda:0\n",
            "1815                (16,) torch.float32         0.00     cuda:0\n",
            "1816                (16,) torch.float32         0.00     cuda:0\n",
            "1817                (16,) torch.float32         0.00     cuda:0\n",
            "1818                (16,) torch.float32         0.00     cuda:0\n",
            "1819                (16,) torch.float32         0.00     cuda:0\n",
            "1820                (16,) torch.float32         0.00     cuda:0\n",
            "1821                (16,) torch.float32         0.00     cuda:0\n",
            "1822                (16,) torch.float32         0.00     cuda:0\n",
            "1823                (16,) torch.float32         0.00     cuda:0\n",
            "1824                (16,) torch.float32         0.00     cuda:0\n",
            "1825                (16,) torch.float32         0.00     cuda:0\n",
            "1826                (16,) torch.float32         0.00     cuda:0\n",
            "1827                (16,) torch.float32         0.00     cuda:0\n",
            "1828                (16,) torch.float32         0.00     cuda:0\n",
            "1829                (16,) torch.float32         0.00     cuda:0\n",
            "1830                (16,) torch.float32         0.00     cuda:0\n",
            "1831                (16,) torch.float32         0.00     cuda:0\n",
            "1832                (16,) torch.float32         0.00     cuda:0\n",
            "1833                (16,) torch.float32         0.00     cuda:0\n",
            "1834                (16,) torch.float32         0.00     cuda:0\n",
            "1835                (16,) torch.float32         0.00     cuda:0\n",
            "1836                (16,) torch.float32         0.00     cuda:0\n",
            "1837                (16,) torch.float32         0.00     cuda:0\n",
            "1838                (16,) torch.float32         0.00     cuda:0\n",
            "1839                (16,) torch.float32         0.00     cuda:0\n",
            "1840                (16,) torch.float32         0.00     cuda:0\n",
            "1841                (16,) torch.float32         0.00     cuda:0\n",
            "1842                (16,) torch.float32         0.00     cuda:0\n",
            "1843                (16,) torch.float32         0.00     cuda:0\n",
            "1844                (16,) torch.float32         0.00     cuda:0\n",
            "1845                (16,) torch.float32         0.00     cuda:0\n",
            "1846                (16,) torch.float32         0.00     cuda:0\n",
            "1847                (16,) torch.float32         0.00     cuda:0\n",
            "1848                (16,) torch.float32         0.00     cuda:0\n",
            "1849                (16,) torch.float32         0.00     cuda:0\n",
            "1850                (16,) torch.float32         0.00     cuda:0\n",
            "1851                (16,) torch.float32         0.00     cuda:0\n",
            "1852                (16,) torch.float32         0.00     cuda:0\n",
            "1853                (16,) torch.float32         0.00     cuda:0\n",
            "1854                (16,) torch.float32         0.00     cuda:0\n",
            "1855                (16,) torch.float32         0.00     cuda:0\n",
            "1856                (16,) torch.float32         0.00     cuda:0\n",
            "1857                (16,) torch.float32         0.00     cuda:0\n",
            "1858                (16,) torch.float32         0.00     cuda:0\n",
            "1859                (16,) torch.float32         0.00     cuda:0\n",
            "1860                (16,) torch.float32         0.00     cuda:0\n",
            "1861                (16,) torch.float32         0.00     cuda:0\n",
            "1862                (16,) torch.float32         0.00     cuda:0\n",
            "1863                (16,) torch.float32         0.00     cuda:0\n",
            "1864                (16,) torch.float32         0.00     cuda:0\n",
            "1865                (16,) torch.float32         0.00     cuda:0\n",
            "1866                (16,) torch.float32         0.00     cuda:0\n",
            "1867                (16,) torch.float32         0.00     cuda:0\n",
            "1868                (16,) torch.float32         0.00     cuda:0\n",
            "1869                (16,) torch.float32         0.00     cuda:0\n",
            "1870                (16,) torch.float32         0.00     cuda:0\n",
            "1871                (16,) torch.float32         0.00     cuda:0\n",
            "1872              (1, 16) torch.float32         0.00     cuda:0\n",
            "1873                (16,) torch.float32         0.00     cuda:0\n",
            "1874                (16,) torch.float32         0.00     cuda:0\n",
            "1875                (16,) torch.float32         0.00     cuda:0\n",
            "1876                (16,) torch.float32         0.00     cuda:0\n",
            "1877                (16,) torch.float32         0.00     cuda:0\n",
            "1878                (16,) torch.float32         0.00     cuda:0\n",
            "1879           (1, 1, 16) torch.float32         0.00     cuda:0\n",
            "1880                (16,) torch.float32         0.00     cuda:0\n",
            "1881                (16,) torch.float32         0.00     cuda:0\n",
            "1882                (16,) torch.float32         0.00     cuda:0\n",
            "1883                (16,) torch.float32         0.00     cuda:0\n",
            "1884                (16,) torch.float32         0.00     cuda:0\n",
            "1885                (16,) torch.float32         0.00     cuda:0\n",
            "1886                (16,) torch.float32         0.00     cuda:0\n",
            "1887                (16,) torch.float32         0.00     cuda:0\n",
            "1888                (16,) torch.float32         0.00     cuda:0\n",
            "1889                (16,) torch.float32         0.00     cuda:0\n",
            "1890                (16,) torch.float32         0.00     cuda:0\n",
            "1891                (16,) torch.float32         0.00     cuda:0\n",
            "1892                (16,) torch.float32         0.00     cuda:0\n",
            "1893                (16,) torch.float32         0.00     cuda:0\n",
            "1894                (16,) torch.float32         0.00     cuda:0\n",
            "1895                (16,) torch.float32         0.00     cuda:0\n",
            "1896                (16,) torch.float32         0.00     cuda:0\n",
            "1897                (16,) torch.float32         0.00     cuda:0\n",
            "1898                (16,) torch.float32         0.00     cuda:0\n",
            "1899                (16,) torch.float32         0.00     cuda:0\n",
            "1900                (16,) torch.float32         0.00     cuda:0\n",
            "1901                (16,) torch.float32         0.00     cuda:0\n",
            "1902                (16,) torch.float32         0.00     cuda:0\n",
            "1903                (16,) torch.float32         0.00     cuda:0\n",
            "1904                (16,) torch.float32         0.00     cuda:0\n",
            "1905                (16,) torch.float32         0.00     cuda:0\n",
            "1906                (16,) torch.float32         0.00     cuda:0\n",
            "1907                (16,) torch.float32         0.00     cuda:0\n",
            "1908                (16,) torch.float32         0.00     cuda:0\n",
            "1909                (16,) torch.float32         0.00     cuda:0\n",
            "1910                (16,) torch.float32         0.00     cuda:0\n",
            "1911                (16,) torch.float32         0.00     cuda:0\n",
            "1912                (16,) torch.float32         0.00     cuda:0\n",
            "1913                (16,) torch.float32         0.00     cuda:0\n",
            "1914                (16,) torch.float32         0.00     cuda:0\n",
            "1915                (16,) torch.float32         0.00     cuda:0\n",
            "1916                (16,) torch.float32         0.00     cuda:0\n",
            "1917                (16,) torch.float32         0.00     cuda:0\n",
            "1918                (16,) torch.float32         0.00     cuda:0\n",
            "1919                (16,) torch.float32         0.00     cuda:0\n",
            "1920                (16,) torch.float32         0.00     cuda:0\n",
            "1921                (16,) torch.float32         0.00     cuda:0\n",
            "1922                (16,) torch.float32         0.00     cuda:0\n",
            "1923                (16,) torch.float32         0.00     cuda:0\n",
            "1924                (16,) torch.float32         0.00     cuda:0\n",
            "1925                (16,) torch.float32         0.00     cuda:0\n",
            "1926                (16,) torch.float32         0.00     cuda:0\n",
            "1927                (16,) torch.float32         0.00     cuda:0\n",
            "1928                (16,) torch.float32         0.00     cuda:0\n",
            "1929                (16,) torch.float32         0.00     cuda:0\n",
            "1930                (16,) torch.float32         0.00     cuda:0\n",
            "1931                (16,) torch.float32         0.00     cuda:0\n",
            "1932                (16,) torch.float32         0.00     cuda:0\n",
            "1933                (16,) torch.float32         0.00     cuda:0\n",
            "1934                (16,) torch.float32         0.00     cuda:0\n",
            "1935                (16,) torch.float32         0.00     cuda:0\n",
            "1936                (16,) torch.float32         0.00     cuda:0\n",
            "1937                (16,) torch.float32         0.00     cuda:0\n",
            "1938                (16,) torch.float32         0.00     cuda:0\n",
            "1939                (16,) torch.float32         0.00     cuda:0\n",
            "1940                (16,) torch.float32         0.00     cuda:0\n",
            "1941                (16,) torch.float32         0.00     cuda:0\n",
            "1942                (16,) torch.float32         0.00     cuda:0\n",
            "1943                (16,) torch.float32         0.00     cuda:0\n",
            "1944                (16,) torch.float32         0.00     cuda:0\n",
            "1945                (16,) torch.float32         0.00     cuda:0\n",
            "1946                (16,) torch.float32         0.00     cuda:0\n",
            "1947                (16,) torch.float32         0.00     cuda:0\n",
            "1948                (16,) torch.float32         0.00     cuda:0\n",
            "1949                (16,) torch.float32         0.00     cuda:0\n",
            "1950                (16,) torch.float32         0.00     cuda:0\n",
            "1951                (16,) torch.float32         0.00     cuda:0\n",
            "1952                (16,) torch.float32         0.00     cuda:0\n",
            "1953                (16,) torch.float32         0.00     cuda:0\n",
            "1954                (16,) torch.float32         0.00     cuda:0\n",
            "1955                (16,) torch.float32         0.00     cuda:0\n",
            "1956                (16,) torch.float32         0.00     cuda:0\n",
            "1957              (1, 16) torch.float32         0.00     cuda:0\n",
            "1958                (16,) torch.float32         0.00     cuda:0\n",
            "1959                (16,) torch.float32         0.00     cuda:0\n",
            "1960           (1, 1, 16) torch.float32         0.00     cuda:0\n",
            "1961              (1, 16) torch.float32         0.00     cuda:0\n",
            "1962              (1, 16) torch.float32         0.00     cuda:0\n",
            "1963              (1, 16) torch.float32         0.00     cuda:0\n",
            "1964                (16,) torch.float32         0.00     cuda:0\n",
            "1965               (5, 3) torch.float32         0.00     cuda:0\n",
            "1966                 (5,) torch.float32         0.00     cuda:0\n",
            "1967               (1, 2) torch.int64         0.00     cuda:0\n",
            "1968                 (2,) torch.int64         0.00     cuda:0\n",
            "1969               (1, 2) torch.int64         0.00     cuda:0\n",
            "1970               (1, 2) torch.int64         0.00     cuda:0\n",
            "1971                 (2,) torch.int64         0.00     cuda:0\n",
            "1972               (1, 2) torch.int64         0.00     cuda:0\n",
            "1973               (1, 2) torch.int64         0.00     cuda:0\n",
            "1974                 (2,) torch.int64         0.00     cuda:0\n",
            "1975               (1, 2) torch.int64         0.00     cuda:0\n",
            "1976               (1, 2) torch.int64         0.00     cuda:0\n",
            "1977               (1, 2) torch.int64         0.00     cuda:0\n",
            "1978                 (3,) torch.float32         0.00     cuda:0\n",
            "1979               (1, 2) torch.float32         0.00     cuda:0\n",
            "1980         (1, 1, 2, 1) torch.float32         0.00     cuda:0\n",
            "1981               (1, 2) torch.float32         0.00     cuda:0\n",
            "1982         (1, 1, 2, 1) torch.float32         0.00     cuda:0\n",
            "1983                 (1,) torch.int64         0.00     cuda:0\n",
            "1984               (1, 2) torch.float32         0.00     cuda:0\n",
            "1985         (1, 1, 2, 1) torch.float32         0.00     cuda:0\n",
            "1986                   () torch.float32         0.00     cuda:0\n",
            "1987                   () torch.float32         0.00     cuda:0\n",
            "------------------------------------------------------------\n",
            "Total GPU tensor memory: 3016.37 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch\n",
        "gc.collect(); torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "StNKGN529aU2"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}